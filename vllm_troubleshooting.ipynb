{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Entity Extraction Performance Troubleshooting\n",
    "\n",
    "This notebook provides comprehensive tools for troubleshooting entity extraction performance issues using vLLM as a Python package.\n",
    "\n",
    "## Goals:\n",
    "- Direct control over all vLLM parameters\n",
    "- Identify why extraction fails at ~5000 characters\n",
    "- Find optimal settings for dual NVIDIA A40 GPUs (46GB VRAM each)\n",
    "- Compare Granite 3.3 2B vs Qwen 2.5 72B performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "```bash\n",
    "# Run in terminal first:\n",
    "conda create -n vllm-test python=3.12 -y\n",
    "conda activate vllm-test\n",
    "VLLM_USE_PRECOMPILED=1 pip install vllm\n",
    "pip install jupyter ipykernel pandas matplotlib psutil nvidia-ml-py3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# vLLM imports\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.utils import is_hip\n",
    "\n",
    "# GPU monitoring\n",
    "try:\n",
    "    import nvidia_ml_py3 as nvml\n",
    "    nvml.nvmlInit()\n",
    "    GPU_AVAILABLE = True\n",
    "except:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"Warning: NVIDIA GPU monitoring not available\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Monitoring Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Container for performance metrics\"\"\"\n",
    "    model_name: str\n",
    "    input_chars: int\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    total_time: float\n",
    "    time_to_first_token: float\n",
    "    tokens_per_second: float\n",
    "    gpu_memory_used: float\n",
    "    gpu_utilization: float\n",
    "    finish_reason: str\n",
    "    config: Dict\n",
    "    \n",
    "class GPUMonitor:\n",
    "    \"\"\"Monitor GPU memory and utilization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gpu_memory():\n",
    "        \"\"\"Get current GPU memory usage in GB\"\"\"\n",
    "        if not GPU_AVAILABLE:\n",
    "            return {\"gpu_0\": 0, \"gpu_1\": 0}\n",
    "        \n",
    "        memory_info = {}\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            handle = nvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            info = nvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            memory_info[f\"gpu_{i}\"] = info.used / 1024**3  # Convert to GB\n",
    "        return memory_info\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gpu_utilization():\n",
    "        \"\"\"Get current GPU utilization percentage\"\"\"\n",
    "        if not GPU_AVAILABLE:\n",
    "            return {\"gpu_0\": 0, \"gpu_1\": 0}\n",
    "        \n",
    "        util_info = {}\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            handle = nvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            util = nvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            util_info[f\"gpu_{i}\"] = util.gpu\n",
    "        return util_info\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_gpu_status():\n",
    "        \"\"\"Print current GPU status\"\"\"\n",
    "        memory = GPUMonitor.get_gpu_memory()\n",
    "        util = GPUMonitor.get_gpu_utilization()\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: Memory: {memory[f'gpu_{i}']:.2f}GB, Utilization: {util[f'gpu_{i}']}%\")\n",
    "\n",
    "# Test GPU monitoring\n",
    "GPUMonitor.print_gpu_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. vLLM Configuration Presets\n",
    "\n",
    "Different configuration presets for various optimization goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMConfigs:\n",
    "    \"\"\"Pre-defined vLLM configurations for different scenarios\"\"\"\n",
    "    \n",
    "    # Baseline configuration\n",
    "    BASELINE = {\n",
    "        \"gpu_memory_utilization\": 0.9,\n",
    "        \"max_model_len\": None,  # Use model default\n",
    "        \"dtype\": \"auto\",\n",
    "    }\n",
    "    \n",
    "    # Optimized for throughput\n",
    "    HIGH_THROUGHPUT = {\n",
    "        \"gpu_memory_utilization\": 0.95,\n",
    "        \"max_model_len\": 16384,\n",
    "        \"max_num_seqs\": 256,\n",
    "        \"max_num_batched_tokens\": 4096,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"dtype\": \"float16\",\n",
    "    }\n",
    "    \n",
    "    # Optimized for low latency\n",
    "    LOW_LATENCY = {\n",
    "        \"gpu_memory_utilization\": 0.9,\n",
    "        \"max_model_len\": 8192,\n",
    "        \"max_num_seqs\": 64,\n",
    "        \"max_num_batched_tokens\": 512,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"dtype\": \"float16\",\n",
    "    }\n",
    "    \n",
    "    # Memory constrained (for large models)\n",
    "    MEMORY_OPTIMIZED = {\n",
    "        \"gpu_memory_utilization\": 0.95,\n",
    "        \"max_model_len\": 4096,\n",
    "        \"max_num_seqs\": 32,\n",
    "        \"max_num_batched_tokens\": 1024,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"swap_space\": 16,  # GB of CPU swap space\n",
    "        \"dtype\": \"float16\",\n",
    "    }\n",
    "    \n",
    "    # For debugging 5000+ char issues\n",
    "    DEBUG_LONG_CONTEXT = {\n",
    "        \"gpu_memory_utilization\": 0.95,\n",
    "        \"max_model_len\": 32768,  # Extended context\n",
    "        \"max_num_seqs\": 16,  # Reduced for more KV cache\n",
    "        \"max_num_batched_tokens\": 8192,  # Large batch for long docs\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"block_size\": 16,\n",
    "        \"dtype\": \"float16\",\n",
    "    }\n",
    "\n",
    "# Print available configs\n",
    "print(\"Available configurations:\")\n",
    "for name in dir(vLLMConfigs):\n",
    "    if not name.startswith('_') and name.isupper():\n",
    "        print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Manage vLLM model loading and unloading\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_model = None\n",
    "        self.current_config = None\n",
    "    \n",
    "    def load_granite_2b(self, config_name: str = \"BASELINE\"):\n",
    "        \"\"\"Load IBM Granite 3.3 2B model\"\"\"\n",
    "        self.unload_model()\n",
    "        \n",
    "        config = getattr(vLLMConfigs, config_name).copy()\n",
    "        print(f\"Loading Granite 3.3 2B with {config_name} config...\")\n",
    "        print(f\"Config: {json.dumps(config, indent=2)}\")\n",
    "        \n",
    "        # Granite specific settings\n",
    "        if config[\"max_model_len\"] is None:\n",
    "            config[\"max_model_len\"] = 128000  # Granite supports 128K context\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.current_model = LLM(\n",
    "            model=\"ibm-granite/granite-3.3-2b-instruct\",\n",
    "            **config\n",
    "        )\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        self.current_config = config\n",
    "        print(f\"Model loaded in {load_time:.2f} seconds\")\n",
    "        GPUMonitor.print_gpu_status()\n",
    "        return self.current_model\n",
    "    \n",
    "    def load_qwen_72b(self, config_name: str = \"MEMORY_OPTIMIZED\"):\n",
    "        \"\"\"Load Qwen 2.5 72B model with tensor parallelism\"\"\"\n",
    "        self.unload_model()\n",
    "        \n",
    "        config = getattr(vLLMConfigs, config_name).copy()\n",
    "        print(f\"Loading Qwen 2.5 72B with {config_name} config...\")\n",
    "        \n",
    "        # Qwen 72B specific settings\n",
    "        if config[\"max_model_len\"] is None:\n",
    "            config[\"max_model_len\"] = 32768  # Qwen default\n",
    "        \n",
    "        # Add tensor parallelism for dual GPUs\n",
    "        config[\"tensor_parallel_size\"] = 2\n",
    "        config[\"distributed_executor_backend\"] = \"mp\"\n",
    "        \n",
    "        print(f\"Config: {json.dumps(config, indent=2)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.current_model = LLM(\n",
    "            model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "            **config\n",
    "        )\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        self.current_config = config\n",
    "        print(f\"Model loaded in {load_time:.2f} seconds\")\n",
    "        GPUMonitor.print_gpu_status()\n",
    "        return self.current_model\n",
    "    \n",
    "    def load_custom(self, model_name: str, config: Dict):\n",
    "        \"\"\"Load a custom model with custom configuration\"\"\"\n",
    "        self.unload_model()\n",
    "        \n",
    "        print(f\"Loading {model_name} with custom config...\")\n",
    "        print(f\"Config: {json.dumps(config, indent=2)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.current_model = LLM(model=model_name, **config)\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        self.current_config = config\n",
    "        print(f\"Model loaded in {load_time:.2f} seconds\")\n",
    "        GPUMonitor.print_gpu_status()\n",
    "        return self.current_model\n",
    "    \n",
    "    def unload_model(self):\n",
    "        \"\"\"Unload current model and free GPU memory\"\"\"\n",
    "        if self.current_model is not None:\n",
    "            print(\"Unloading current model...\")\n",
    "            del self.current_model\n",
    "            self.current_model = None\n",
    "            self.current_config = None\n",
    "            \n",
    "            # Force garbage collection and clear CUDA cache\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            print(\"Model unloaded\")\n",
    "            GPUMonitor.print_gpu_status()\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = ModelManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entity Extraction Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtractionTester:\n",
    "    \"\"\"Test entity extraction with various document sizes and configurations\"\"\"\n",
    "    \n",
    "    def __init__(self, model_manager: ModelManager):\n",
    "        self.model_manager = model_manager\n",
    "        self.results = []\n",
    "    \n",
    "    def create_test_prompt(self, text: str, char_count: int) -> str:\n",
    "        \"\"\"Create entity extraction prompt\"\"\"\n",
    "        # Truncate or pad text to desired character count\n",
    "        if len(text) > char_count:\n",
    "            text = text[:char_count]\n",
    "        elif len(text) < char_count:\n",
    "            # Repeat text to reach desired length\n",
    "            multiplier = (char_count // len(text)) + 1\n",
    "            text = (text * multiplier)[:char_count]\n",
    "        \n",
    "        prompt = f\"\"\"Extract all legal entities from the following document.\n",
    "Return the entities in JSON format with these categories:\n",
    "- persons\n",
    "- organizations\n",
    "- locations\n",
    "- dates\n",
    "- monetary_amounts\n",
    "- case_numbers\n",
    "\n",
    "Document:\n",
    "{text}\n",
    "\n",
    "Extracted entities:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def test_extraction(self, \n",
    "                       text: str,\n",
    "                       char_counts: List[int],\n",
    "                       sampling_params: Optional[SamplingParams] = None) -> pd.DataFrame:\n",
    "        \"\"\"Test extraction with different document sizes\"\"\"\n",
    "        \n",
    "        if self.model_manager.current_model is None:\n",
    "            raise ValueError(\"No model loaded. Use model_manager to load a model first.\")\n",
    "        \n",
    "        if sampling_params is None:\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.1,\n",
    "                top_p=0.95,\n",
    "                max_tokens=2048,\n",
    "                repetition_penalty=1.05,\n",
    "            )\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for char_count in char_counts:\n",
    "            print(f\"\\nTesting with {char_count} characters...\")\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = self.create_test_prompt(text, char_count)\n",
    "            \n",
    "            # Monitor GPU before generation\n",
    "            gpu_before = GPUMonitor.get_gpu_memory()\n",
    "            \n",
    "            try:\n",
    "                # Generate\n",
    "                start_time = time.time()\n",
    "                outputs = self.model_manager.current_model.generate(\n",
    "                    [prompt], \n",
    "                    sampling_params\n",
    "                )\n",
    "                total_time = time.time() - start_time\n",
    "                \n",
    "                # Get output\n",
    "                output = outputs[0]\n",
    "                generated_text = output.outputs[0].text\n",
    "                \n",
    "                # Calculate metrics\n",
    "                input_tokens = len(output.prompt_token_ids)\n",
    "                output_tokens = len(output.outputs[0].token_ids)\n",
    "                tokens_per_second = output_tokens / total_time if total_time > 0 else 0\n",
    "                \n",
    "                # Monitor GPU after generation\n",
    "                gpu_after = GPUMonitor.get_gpu_memory()\n",
    "                gpu_used = max(gpu_after.values()) - max(gpu_before.values())\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    \"char_count\": char_count,\n",
    "                    \"input_tokens\": input_tokens,\n",
    "                    \"output_tokens\": output_tokens,\n",
    "                    \"total_time\": total_time,\n",
    "                    \"tokens_per_second\": tokens_per_second,\n",
    "                    \"gpu_memory_delta\": gpu_used,\n",
    "                    \"finish_reason\": output.outputs[0].finish_reason,\n",
    "                    \"success\": True,\n",
    "                    \"error\": None,\n",
    "                    \"output_preview\": generated_text[:200]\n",
    "                }\n",
    "                \n",
    "                print(f\"  ✓ Success: {input_tokens} input tokens, {output_tokens} output tokens\")\n",
    "                print(f\"  Time: {total_time:.2f}s, Speed: {tokens_per_second:.1f} tokens/s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                result = {\n",
    "                    \"char_count\": char_count,\n",
    "                    \"input_tokens\": 0,\n",
    "                    \"output_tokens\": 0,\n",
    "                    \"total_time\": 0,\n",
    "                    \"tokens_per_second\": 0,\n",
    "                    \"gpu_memory_delta\": 0,\n",
    "                    \"finish_reason\": \"error\",\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e),\n",
    "                    \"output_preview\": None\n",
    "                }\n",
    "                print(f\"  ✗ Error: {str(e)}\")\n",
    "            \n",
    "            results.append(result)\n",
    "            self.results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def plot_results(self, df: pd.DataFrame):\n",
    "        \"\"\"Visualize test results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # Success rate\n",
    "        ax = axes[0, 0]\n",
    "        success_data = df.groupby('char_count')['success'].mean() * 100\n",
    "        ax.plot(success_data.index, success_data.values, 'o-')\n",
    "        ax.set_xlabel('Character Count')\n",
    "        ax.set_ylabel('Success Rate (%)')\n",
    "        ax.set_title('Extraction Success Rate')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Token usage\n",
    "        ax = axes[0, 1]\n",
    "        ax.plot(df['char_count'], df['input_tokens'], 'o-', label='Input Tokens')\n",
    "        ax.plot(df['char_count'], df['output_tokens'], 's-', label='Output Tokens')\n",
    "        ax.set_xlabel('Character Count')\n",
    "        ax.set_ylabel('Token Count')\n",
    "        ax.set_title('Token Usage')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Processing time\n",
    "        ax = axes[1, 0]\n",
    "        ax.plot(df['char_count'], df['total_time'], 'o-')\n",
    "        ax.set_xlabel('Character Count')\n",
    "        ax.set_ylabel('Time (seconds)')\n",
    "        ax.set_title('Processing Time')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Tokens per second\n",
    "        ax = axes[1, 1]\n",
    "        ax.plot(df['char_count'], df['tokens_per_second'], 'o-')\n",
    "        ax.set_xlabel('Character Count')\n",
    "        ax.set_ylabel('Tokens/Second')\n",
    "        ax.set_title('Generation Speed')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize tester\n",
    "tester = EntityExtractionTester(model_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration Comparison Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigComparison:\n",
    "    \"\"\"Compare different vLLM configurations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_configs(model_manager: ModelManager,\n",
    "                       model_loader: str,  # 'load_granite_2b' or 'load_qwen_72b'\n",
    "                       configs: List[str],\n",
    "                       test_text: str,\n",
    "                       test_char_count: int = 5000) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple configurations\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for config_name in configs:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Testing {config_name} configuration\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Load model with config\n",
    "            loader = getattr(model_manager, model_loader)\n",
    "            loader(config_name)\n",
    "            \n",
    "            # Test extraction\n",
    "            tester = EntityExtractionTester(model_manager)\n",
    "            df = tester.test_extraction(test_text, [test_char_count])\n",
    "            \n",
    "            if len(df) > 0:\n",
    "                row = df.iloc[0].to_dict()\n",
    "                row['config'] = config_name\n",
    "                row['model'] = model_loader\n",
    "                results.append(row)\n",
    "            \n",
    "            # Unload to free memory\n",
    "            model_manager.unload_model()\n",
    "        \n",
    "        comparison_df = pd.DataFrame(results)\n",
    "        return comparison_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_comparison(df: pd.DataFrame):\n",
    "        \"\"\"Plot configuration comparison\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Processing time comparison\n",
    "        ax = axes[0]\n",
    "        ax.bar(df['config'], df['total_time'])\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Time (seconds)')\n",
    "        ax.set_title('Processing Time')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Tokens per second\n",
    "        ax = axes[1]\n",
    "        ax.bar(df['config'], df['tokens_per_second'])\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Tokens/Second')\n",
    "        ax.set_title('Generation Speed')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Success rate\n",
    "        ax = axes[2]\n",
    "        success_values = df['success'].astype(int) * 100\n",
    "        colors = ['green' if s else 'red' for s in df['success']]\n",
    "        ax.bar(df['config'], success_values, color=colors)\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Success (%)')\n",
    "        ax.set_title('Extraction Success')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.set_ylim(0, 110)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Sampling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingStrategies:\n",
    "    \"\"\"Different sampling strategies for entity extraction\"\"\"\n",
    "    \n",
    "    # Deterministic extraction\n",
    "    DETERMINISTIC = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=2048,\n",
    "        repetition_penalty=1.0,\n",
    "    )\n",
    "    \n",
    "    # Slightly creative but consistent\n",
    "    CONSISTENT = SamplingParams(\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        max_tokens=2048,\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "    \n",
    "    # Balanced extraction\n",
    "    BALANCED = SamplingParams(\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        max_tokens=2048,\n",
    "        repetition_penalty=1.1,\n",
    "        frequency_penalty=0.1,\n",
    "        presence_penalty=0.1,\n",
    "    )\n",
    "    \n",
    "    # For structured output\n",
    "    STRUCTURED = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=4096,\n",
    "        stop=[\"</entities>\", \"\\n\\n\\n\", \"```\\n\"],\n",
    "        skip_special_tokens=False,\n",
    "    )\n",
    "    \n",
    "    # Long document optimized\n",
    "    LONG_DOCUMENT = SamplingParams(\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        max_tokens=8192,  # Extended for long outputs\n",
    "        repetition_penalty=1.02,  # Lower to allow entity repetition\n",
    "    )\n",
    "\n",
    "print(\"Available sampling strategies:\")\n",
    "for name in dir(SamplingStrategies):\n",
    "    if not name.startswith('_') and name.isupper():\n",
    "        strategy = getattr(SamplingStrategies, name)\n",
    "        print(f\"  - {name}: temp={strategy.temperature}, max_tokens={strategy.max_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Troubleshooting 5000+ Character Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_5k_char_issue(model_manager: ModelManager, test_text: str):\n",
    "    \"\"\"Comprehensive diagnosis of the 5000+ character extraction issue\"\"\"\n",
    "    \n",
    "    print(\"Diagnosing 5000+ character extraction issue...\\n\")\n",
    "    \n",
    "    # Test points around the 5000 character boundary\n",
    "    test_points = [3000, 4000, 4500, 4900, 5000, 5100, 5500, 6000, 7000]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Test with different configurations\n",
    "    configs_to_test = [\n",
    "        (\"BASELINE\", vLLMConfigs.BASELINE),\n",
    "        (\"DEBUG_LONG_CONTEXT\", vLLMConfigs.DEBUG_LONG_CONTEXT),\n",
    "    ]\n",
    "    \n",
    "    for config_name, config in configs_to_test:\n",
    "        print(f\"\\nTesting with {config_name} configuration:\")\n",
    "        print(f\"max_model_len: {config.get('max_model_len', 'default')}\")\n",
    "        print(f\"max_num_batched_tokens: {config.get('max_num_batched_tokens', 'default')}\")\n",
    "        print(f\"enable_chunked_prefill: {config.get('enable_chunked_prefill', False)}\")\n",
    "        print()\n",
    "        \n",
    "        # Load model\n",
    "        model_manager.load_granite_2b(config_name)\n",
    "        \n",
    "        for char_count in test_points:\n",
    "            # Create test prompt\n",
    "            prompt = tester.create_test_prompt(test_text, char_count)\n",
    "            \n",
    "            # Count tokens\n",
    "            try:\n",
    "                # Try to tokenize to get token count\n",
    "                # Note: This is approximate as vLLM uses its own tokenizer\n",
    "                approx_tokens = len(prompt) // 4  # Rough estimate: 4 chars per token\n",
    "                \n",
    "                # Test generation\n",
    "                sampling_params = SamplingStrategies.LONG_DOCUMENT\n",
    "                \n",
    "                start_time = time.time()\n",
    "                outputs = model_manager.current_model.generate(\n",
    "                    [prompt], \n",
    "                    sampling_params\n",
    "                )\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                output = outputs[0]\n",
    "                success = True\n",
    "                error = None\n",
    "                input_tokens = len(output.prompt_token_ids)\n",
    "                output_tokens = len(output.outputs[0].token_ids)\n",
    "                finish_reason = output.outputs[0].finish_reason\n",
    "                \n",
    "            except Exception as e:\n",
    "                success = False\n",
    "                error = str(e)\n",
    "                input_tokens = approx_tokens\n",
    "                output_tokens = 0\n",
    "                finish_reason = \"error\"\n",
    "                elapsed = 0\n",
    "            \n",
    "            result = {\n",
    "                \"config\": config_name,\n",
    "                \"char_count\": char_count,\n",
    "                \"approx_tokens\": approx_tokens,\n",
    "                \"actual_input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"success\": success,\n",
    "                \"finish_reason\": finish_reason,\n",
    "                \"time\": elapsed,\n",
    "                \"error\": error\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            status = \"✓\" if success else \"✗\"\n",
    "            print(f\"{status} {char_count:5d} chars | {input_tokens:5d} tokens | \"\n",
    "                  f\"Reason: {finish_reason:10s} | Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        # Unload model\n",
    "        model_manager.unload_model()\n",
    "    \n",
    "    # Analyze results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DIAGNOSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find failure point\n",
    "    for config_name in df['config'].unique():\n",
    "        config_df = df[df['config'] == config_name]\n",
    "        failures = config_df[~config_df['success']]\n",
    "        \n",
    "        print(f\"\\n{config_name}:\")\n",
    "        if len(failures) > 0:\n",
    "            first_failure = failures.iloc[0]\n",
    "            print(f\"  First failure at: {first_failure['char_count']} characters\")\n",
    "            print(f\"  Token count: {first_failure['actual_input_tokens']}\")\n",
    "            print(f\"  Error: {first_failure['error']}\")\n",
    "        else:\n",
    "            print(\"  All tests passed!\")\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Success by character count\n",
    "    ax = axes[0]\n",
    "    for config_name in df['config'].unique():\n",
    "        config_df = df[df['config'] == config_name]\n",
    "        ax.plot(config_df['char_count'], \n",
    "               config_df['success'].astype(int) * 100,\n",
    "               'o-', label=config_name)\n",
    "    ax.set_xlabel('Character Count')\n",
    "    ax.set_ylabel('Success (%)')\n",
    "    ax.set_title('Extraction Success by Document Size')\n",
    "    ax.axvline(x=5000, color='red', linestyle='--', alpha=0.5, label='5000 chars')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Token count vs char count\n",
    "    ax = axes[1]\n",
    "    for config_name in df['config'].unique():\n",
    "        config_df = df[df['config'] == config_name]\n",
    "        ax.plot(config_df['char_count'], \n",
    "               config_df['actual_input_tokens'],\n",
    "               'o-', label=config_name)\n",
    "    ax.set_xlabel('Character Count')\n",
    "    ax.set_ylabel('Token Count')\n",
    "    ax.set_title('Token Usage by Document Size')\n",
    "    ax.axvline(x=5000, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Example Usage - Basic Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample legal text for testing\n",
    "sample_text = \"\"\"\n",
    "UNITED STATES DISTRICT COURT\n",
    "SOUTHERN DISTRICT OF NEW YORK\n",
    "\n",
    "JOHN DOE, individually and on behalf of all others similarly situated,\n",
    "Plaintiff,\n",
    "v.\n",
    "ACME CORPORATION, a Delaware corporation, and JANE SMITH, CEO,\n",
    "Defendants.\n",
    "\n",
    "Case No. 23-CV-1234-ABC\n",
    "\n",
    "COMPLAINT\n",
    "\n",
    "Plaintiff John Doe, by and through undersigned counsel, brings this action against \n",
    "Defendants Acme Corporation and Jane Smith, and alleges as follows:\n",
    "\n",
    "1. This is a class action brought on behalf of all purchasers of Acme Corporation's \n",
    "   products between January 1, 2020 and December 31, 2023.\n",
    "\n",
    "2. The amount in controversy exceeds $5,000,000.00, exclusive of interest and costs.\n",
    "\n",
    "3. On or about March 15, 2023, Defendant Jane Smith made false statements regarding \n",
    "   the safety of Acme's products at the company's headquarters in New York, NY.\n",
    "\"\"\"\n",
    "\n",
    "# Load Granite model with baseline config\n",
    "model = model_manager.load_granite_2b(\"BASELINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with increasing document sizes\n",
    "test_sizes = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
    "\n",
    "results_df = tester.test_extraction(\n",
    "    text=sample_text,\n",
    "    char_counts=test_sizes,\n",
    "    sampling_params=SamplingStrategies.CONSISTENT\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nTest Results Summary:\")\n",
    "print(results_df[['char_count', 'input_tokens', 'output_tokens', \n",
    "                  'total_time', 'tokens_per_second', 'success']])\n",
    "\n",
    "# Plot results\n",
    "tester.plot_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Troubleshooting - 5000 Character Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose the 5000+ character issue\n",
    "diagnosis_df = diagnose_5k_char_issue(model_manager, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Configuration Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different configurations\n",
    "configs_to_compare = [\"BASELINE\", \"HIGH_THROUGHPUT\", \"LOW_LATENCY\", \"DEBUG_LONG_CONTEXT\"]\n",
    "\n",
    "comparison_df = ConfigComparison.compare_configs(\n",
    "    model_manager=model_manager,\n",
    "    model_loader=\"load_granite_2b\",\n",
    "    configs=configs_to_compare,\n",
    "    test_text=sample_text,\n",
    "    test_char_count=5000\n",
    ")\n",
    "\n",
    "print(\"\\nConfiguration Comparison Results:\")\n",
    "print(comparison_df[['config', 'total_time', 'tokens_per_second', 'success']])\n",
    "\n",
    "ConfigComparison.plot_comparison(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Custom Configuration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test a custom configuration\n",
    "custom_config = {\n",
    "    \"gpu_memory_utilization\": 0.98,  # Maximum memory usage\n",
    "    \"max_model_len\": 16384,  # Moderate context length\n",
    "    \"max_num_seqs\": 8,  # Few sequences for more KV cache per sequence\n",
    "    \"max_num_batched_tokens\": 16384,  # Large batch for long documents\n",
    "    \"enable_prefix_caching\": True,\n",
    "    \"enable_chunked_prefill\": True,\n",
    "    \"block_size\": 32,  # Larger blocks\n",
    "    \"dtype\": \"float16\",\n",
    "}\n",
    "\n",
    "# Load with custom config\n",
    "model = model_manager.load_custom(\n",
    "    model_name=\"ibm-granite/granite-3.3-2b-instruct\",\n",
    "    config=custom_config\n",
    ")\n",
    "\n",
    "# Test with problematic size\n",
    "test_df = tester.test_extraction(\n",
    "    text=sample_text,\n",
    "    char_counts=[5000, 5500, 6000],\n",
    "    sampling_params=SamplingStrategies.LONG_DOCUMENT\n",
    ")\n",
    "\n",
    "print(\"\\nCustom Configuration Results:\")\n",
    "print(test_df[['char_count', 'success', 'total_time', 'finish_reason']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Memory and Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_generation_performance(model_manager: ModelManager, \n",
    "                                  prompt: str,\n",
    "                                  sampling_params: SamplingParams):\n",
    "    \"\"\"Detailed performance monitoring during generation\"\"\"\n",
    "    \n",
    "    print(\"Monitoring generation performance...\\n\")\n",
    "    \n",
    "    # Pre-generation status\n",
    "    print(\"Before generation:\")\n",
    "    GPUMonitor.print_gpu_status()\n",
    "    \n",
    "    # CPU memory\n",
    "    process = psutil.Process()\n",
    "    cpu_mem_before = process.memory_info().rss / 1024**3  # GB\n",
    "    print(f\"CPU Memory: {cpu_mem_before:.2f} GB\\n\")\n",
    "    \n",
    "    # Generation with timing\n",
    "    start_time = time.time()\n",
    "    outputs = model_manager.current_model.generate([prompt], sampling_params)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Post-generation status\n",
    "    print(\"\\nAfter generation:\")\n",
    "    GPUMonitor.print_gpu_status()\n",
    "    \n",
    "    cpu_mem_after = process.memory_info().rss / 1024**3\n",
    "    print(f\"CPU Memory: {cpu_mem_after:.2f} GB\")\n",
    "    print(f\"CPU Memory Delta: {cpu_mem_after - cpu_mem_before:.2f} GB\\n\")\n",
    "    \n",
    "    # Output analysis\n",
    "    output = outputs[0]\n",
    "    print(\"Generation Metrics:\")\n",
    "    print(f\"  Input tokens: {len(output.prompt_token_ids)}\")\n",
    "    print(f\"  Output tokens: {len(output.outputs[0].token_ids)}\")\n",
    "    print(f\"  Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"  Tokens/second: {len(output.outputs[0].token_ids) / total_time:.1f}\")\n",
    "    print(f\"  Finish reason: {output.outputs[0].finish_reason}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test with monitoring\n",
    "if model_manager.current_model is not None:\n",
    "    test_prompt = tester.create_test_prompt(sample_text, 5000)\n",
    "    output = monitor_generation_performance(\n",
    "        model_manager,\n",
    "        test_prompt,\n",
    "        SamplingStrategies.CONSISTENT\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Recommendations and Best Practices\n",
    "\n",
    "Based on the tests, here are recommendations for resolving the 5000+ character issue:\n",
    "\n",
    "### Key Findings:\n",
    "1. **Token Limit**: The issue likely occurs when input tokens exceed a threshold\n",
    "2. **KV Cache**: Insufficient KV cache memory for long sequences\n",
    "3. **Batching**: Large documents may exceed max_num_batched_tokens\n",
    "\n",
    "### Recommended Solutions:\n",
    "\n",
    "#### For Granite 3.3 2B:\n",
    "```python\n",
    "config = {\n",
    "    \"gpu_memory_utilization\": 0.95,\n",
    "    \"max_model_len\": 32768,  # Increase from default\n",
    "    \"max_num_seqs\": 16,  # Reduce for more KV cache per sequence\n",
    "    \"max_num_batched_tokens\": 8192,  # Increase for long documents\n",
    "    \"enable_chunked_prefill\": True,\n",
    "    \"enable_prefix_caching\": True,\n",
    "}\n",
    "```\n",
    "\n",
    "#### For Qwen 2.5 72B:\n",
    "```python\n",
    "config = {\n",
    "    \"tensor_parallel_size\": 2,\n",
    "    \"gpu_memory_utilization\": 0.95,\n",
    "    \"max_model_len\": 16384,  # Balance memory and context\n",
    "    \"max_num_seqs\": 8,\n",
    "    \"max_num_batched_tokens\": 4096,\n",
    "    \"enable_chunked_prefill\": True,\n",
    "    \"dtype\": \"float16\",\n",
    "}\n",
    "```\n",
    "\n",
    "### Debugging Steps:\n",
    "1. Monitor token counts at failure point\n",
    "2. Check GPU memory usage patterns\n",
    "3. Test with progressively larger max_model_len\n",
    "4. Enable chunked_prefill for long documents\n",
    "5. Reduce max_num_seqs to allocate more KV cache per sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "model_manager.unload_model()\n",
    "print(\"Notebook execution complete. All models unloaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}