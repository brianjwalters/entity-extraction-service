{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction Troubleshooting Notebook\n",
    "\n",
    "## Purpose\n",
    "This notebook provides comprehensive troubleshooting for entity extraction issues, specifically:\n",
    "- Finding the breaking point where extraction fails (starting from 1000 characters)\n",
    "- Testing with vLLM as a Python package (not HTTP service)\n",
    "- Complete parameter control for optimization\n",
    "- Model comparison (Granite 3.3 2B vs Qwen 2.5 72B)\n",
    "\n",
    "## Workflow\n",
    "1. Setup and Dependencies\n",
    "2. vLLM Configuration\n",
    "3. Document Chunking (BEFORE extraction)\n",
    "4. Prompt Template Management\n",
    "5. Dynamic Breaking Point Detection\n",
    "6. Entity Extraction Strategies (AFTER chunking)\n",
    "7. Performance Analysis\n",
    "8. Automated Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "✓ requests\n",
      "✓ torch\n",
      "✓ transformers\n",
      "✓ tiktoken\n",
      "✓ pandas\n",
      "✓ numpy\n",
      "✓ matplotlib\n",
      "✓ seaborn\n",
      "✓ jinja2\n",
      "✓ pyyaml\n",
      "✓ rich\n",
      "✓ psutil\n",
      "✓ nvidia-ml-py3\n",
      "✓ gpustat\n",
      "✓ tqdm\n",
      "\n",
      "Package installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "# Core packages\n",
    "packages = [\n",
    "    'requests',           # API calls for pattern loading\n",
    "    'torch',              # PyTorch for vLLM\n",
    "    'transformers',       # Model loading\n",
    "    'tiktoken',           # Token counting\n",
    "    'pandas',             # Data analysis\n",
    "    'numpy',              # Numerical operations\n",
    "    'matplotlib',         # Plotting\n",
    "    'seaborn',           # Advanced plotting\n",
    "    'jinja2',            # Template rendering\n",
    "    'pyyaml',            # YAML parsing\n",
    "    'rich',              # Rich console output\n",
    "    'psutil',            # System monitoring\n",
    "    'nvidia-ml-py3',     # GPU monitoring with NVML\n",
    "    'gpustat',           # GPU monitoring\n",
    "    'tqdm',              # Progress bars\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        install_package(package)\n",
    "        print(f\"✓ {package.split('==')[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {package}: {e}\")\n",
    "\n",
    "print(\"\\nPackage installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All modules imported successfully\n",
      "✓ Python: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n",
      "✓ PyTorch: 2.7.1+cu126\n",
      "✓ CUDA Available: True\n",
      "✓ GPU: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "# Import all required modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# System monitoring\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Rich output for formatted console display\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich.progress import track, Progress, SpinnerColumn, BarColumn, TextColumn\n",
    "from rich.syntax import Syntax\n",
    "from rich import print as rprint\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Add notebook modules to path\n",
    "notebook_modules_path = '/srv/luris/be/entity-extraction-service/notebook_modules'\n",
    "if notebook_modules_path not in sys.path:\n",
    "    sys.path.insert(0, notebook_modules_path)\n",
    "\n",
    "print(\"✓ All modules imported successfully\")\n",
    "print(f\"✓ Python: {sys.version}\")\n",
    "print(f\"✓ PyTorch: {torch.__version__}\")\n",
    "print(f\"✓ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All notebook modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import custom notebook modules\n",
    "try:\n",
    "    from vllm_controller import VLLMController\n",
    "    from template_manager import TemplateManager\n",
    "    from chunking_engine import ChunkingEngine, ChunkingStrategy\n",
    "    from performance_analyzer import PerformanceAnalyzer\n",
    "    from extraction_tester import ExtractionTester\n",
    "    print(\"✓ All notebook modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing notebook modules: {e}\")\n",
    "    print(\"Make sure the notebook_modules directory exists with all required files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. vLLM Configuration and Model Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ vLLM configurations defined\n",
      "Available presets: ['minimal', 'balanced', 'aggressive', 'long_context']\n"
     ]
    }
   ],
   "source": [
    "# vLLM Configuration Classes\n",
    "@dataclass\n",
    "class VLLMConfig:\n",
    "    \"\"\"Configuration for vLLM models\"\"\"\n",
    "    model_name: str\n",
    "    gpu_memory_utilization: float = 0.9\n",
    "    max_model_len: Optional[int] = None\n",
    "    max_num_batched_tokens: int = 8192\n",
    "    max_num_seqs: int = 256\n",
    "    enable_chunked_prefill: bool = True\n",
    "    enable_prefix_caching: bool = False\n",
    "    tensor_parallel_size: int = 1\n",
    "    dtype: str = \"auto\"\n",
    "    enforce_eager: bool = False\n",
    "    trust_remote_code: bool = True\n",
    "    \n",
    "# Model Configurations\n",
    "GRANITE_CONFIG = VLLMConfig(\n",
    "    model_name=\"ibm-granite/granite-3.1-2b-instruct\",\n",
    "    gpu_memory_utilization=0.85,\n",
    "    max_model_len=128000,  # Full 128K context\n",
    "    max_num_batched_tokens=16384,\n",
    "    enable_chunked_prefill=True,\n",
    "    dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "QWEN_CONFIG = VLLMConfig(\n",
    "    model_name=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    gpu_memory_utilization=0.98,  # Max utilization for 72B\n",
    "    max_model_len=32768,  # Reduced for memory\n",
    "    max_num_batched_tokens=8192,\n",
    "    tensor_parallel_size=2,  # Use both GPUs\n",
    "    enable_chunked_prefill=True,\n",
    "    dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "# Configuration presets for testing\n",
    "CONFIGS = {\n",
    "    \"minimal\": {\n",
    "        \"gpu_memory_utilization\": 0.7,\n",
    "        \"max_num_batched_tokens\": 512,\n",
    "        \"max_num_seqs\": 1,\n",
    "    },\n",
    "    \"balanced\": {\n",
    "        \"gpu_memory_utilization\": 0.85,\n",
    "        \"max_num_batched_tokens\": 4096,\n",
    "        \"max_num_seqs\": 16,\n",
    "    },\n",
    "    \"aggressive\": {\n",
    "        \"gpu_memory_utilization\": 0.95,\n",
    "        \"max_num_batched_tokens\": 16384,\n",
    "        \"max_num_seqs\": 256,\n",
    "    },\n",
    "    \"long_context\": {\n",
    "        \"gpu_memory_utilization\": 0.98,\n",
    "        \"max_model_len\": 128000,\n",
    "        \"max_num_seqs\": 4,\n",
    "        \"enable_prefix_caching\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ vLLM configurations defined\")\n",
    "print(f\"Available presets: {list(CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                         vLLM Configuration Options                          </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Parameter              </span>┃<span style=\"font-weight: bold\"> Range/Options         </span>┃<span style=\"font-weight: bold\"> Description              </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> gpu_memory_utilization </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.7 - 0.98            </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> GPU memory allocation    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> max_model_len          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 512 - 128000          </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Maximum sequence length  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> max_num_batched_tokens </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 512 - 16384           </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Max tokens per batch     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> max_num_seqs           </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 1 - 256               </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Max concurrent sequences </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> enable_chunked_prefill </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> True/False            </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Enable chunked prefill   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> enable_prefix_caching  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> True/False            </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Enable KV cache reuse    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> tensor_parallel_size   </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 1 - 2                 </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> GPU parallelism          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> dtype                  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> auto/float16/bfloat16 </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Model precision          </span>│\n",
       "└────────────────────────┴───────────────────────┴──────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                         vLLM Configuration Options                          \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mParameter             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRange/Options        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDescription             \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mgpu_memory_utilization\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.7 - 0.98           \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mGPU memory allocation   \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mmax_model_len         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m512 - 128000         \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mMaximum sequence length \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mmax_num_batched_tokens\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m512 - 16384          \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mMax tokens per batch    \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mmax_num_seqs          \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1 - 256              \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mMax concurrent sequences\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36menable_chunked_prefill\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrue/False           \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mEnable chunked prefill  \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36menable_prefix_caching \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrue/False           \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mEnable KV cache reuse   \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtensor_parallel_size  \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1 - 2                \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mGPU parallelism         \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mdtype                 \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mauto/float16/bfloat16\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mModel precision         \u001b[0m\u001b[33m \u001b[0m│\n",
       "└────────────────────────┴───────────────────────┴──────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize vLLM Controller\n",
    "vllm_controller = VLLMController()\n",
    "\n",
    "# Display available configurations\n",
    "table = Table(title=\"vLLM Configuration Options\")\n",
    "table.add_column(\"Parameter\", style=\"cyan\")\n",
    "table.add_column(\"Range/Options\", style=\"green\")\n",
    "table.add_column(\"Description\", style=\"yellow\")\n",
    "\n",
    "table.add_row(\"gpu_memory_utilization\", \"0.7 - 0.98\", \"GPU memory allocation\")\n",
    "table.add_row(\"max_model_len\", \"512 - 128000\", \"Maximum sequence length\")\n",
    "table.add_row(\"max_num_batched_tokens\", \"512 - 16384\", \"Max tokens per batch\")\n",
    "table.add_row(\"max_num_seqs\", \"1 - 256\", \"Max concurrent sequences\")\n",
    "table.add_row(\"enable_chunked_prefill\", \"True/False\", \"Enable chunked prefill\")\n",
    "table.add_row(\"enable_prefix_caching\", \"True/False\", \"Enable KV cache reuse\")\n",
    "table.add_row(\"tensor_parallel_size\", \"1 - 2\", \"GPU parallelism\")\n",
    "table.add_row(\"dtype\", \"auto/float16/bfloat16\", \"Model precision\")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Chunking Engine (Before Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunking engine initialized\n",
      "✓ Test sizes: 31 configurations\n",
      "✓ Range: 100 - 100000 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chunking Engine\n",
    "chunking_engine = ChunkingEngine()\n",
    "\n",
    "# Chunking configuration\n",
    "CHUNKING_CONFIG = {\n",
    "    \"min_chunk_size\": 100,\n",
    "    \"max_chunk_size\": 100000,\n",
    "    \"overlap_percentage\": 10,  # 10% overlap between chunks\n",
    "    \"preserve_boundaries\": True,\n",
    "    \"boundary_types\": [\"word\", \"sentence\", \"paragraph\"]\n",
    "}\n",
    "\n",
    "# Test chunk sizes for breaking point detection\n",
    "TEST_CHUNK_SIZES = [\n",
    "    100, 250, 500, 750,\n",
    "    1000, 1500, 2000, 2500, 3000, 3500,\n",
    "    4000, 4500, 5000, 5500, 6000, 6500,\n",
    "    7000, 7500, 8000, 8500, 9000, 9500,\n",
    "    10000, 15000, 20000, 25000, 30000,\n",
    "    40000, 50000, 75000, 100000\n",
    "]\n",
    "\n",
    "print(f\"✓ Chunking engine initialized\")\n",
    "print(f\"✓ Test sizes: {len(TEST_CHUNK_SIZES)} configurations\")\n",
    "print(f\"✓ Range: {min(TEST_CHUNK_SIZES)} - {max(TEST_CHUNK_SIZES)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample chunk created: 499 characters\n",
      "✓ Chunking functions ready\n"
     ]
    }
   ],
   "source": [
    "# Chunking strategies\n",
    "def create_precise_chunk(text: str, size: int) -> str:\n",
    "    \"\"\"Create a chunk of exact character size\"\"\"\n",
    "    if len(text) <= size:\n",
    "        return text\n",
    "    return text[:size]\n",
    "\n",
    "def create_word_boundary_chunk(text: str, size: int) -> str:\n",
    "    \"\"\"Create a chunk respecting word boundaries\"\"\"\n",
    "    if len(text) <= size:\n",
    "        return text\n",
    "    \n",
    "    chunk = text[:size]\n",
    "    # Find last complete word\n",
    "    last_space = chunk.rfind(' ')\n",
    "    if last_space > 0:\n",
    "        return chunk[:last_space]\n",
    "    return chunk\n",
    "\n",
    "def create_sentence_boundary_chunk(text: str, size: int) -> str:\n",
    "    \"\"\"Create a chunk respecting sentence boundaries\"\"\"\n",
    "    if len(text) <= size:\n",
    "        return text\n",
    "    \n",
    "    chunk = text[:size]\n",
    "    # Find last complete sentence\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]\\s+', chunk)\n",
    "    if len(sentences) > 1:\n",
    "        return '. '.join(sentences[:-1]) + '.'\n",
    "    return chunk\n",
    "\n",
    "# Test chunking\n",
    "sample_text = \"This is a sample text. \" * 100\n",
    "test_chunk = create_word_boundary_chunk(sample_text, 500)\n",
    "print(f\"✓ Sample chunk created: {len(test_chunk)} characters\")\n",
    "print(f\"✓ Chunking functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. API Pattern Loading and Prompt Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────── Configuration ─────────────╮\n",
       "│ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Extraction Configuration</span>               │\n",
       "│ Mode: ai_enhanced                      │\n",
       "│ Strategy: multipass                    │\n",
       "│ Profile: default                       │\n",
       "│ Pass: actors                           │\n",
       "│ API URL: http://10.10.0.87:8007/api/v1 │\n",
       "╰────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────── Configuration ─────────────╮\n",
       "│ \u001b[1;36mExtraction Configuration\u001b[0m               │\n",
       "│ Mode: ai_enhanced                      │\n",
       "│ Strategy: multipass                    │\n",
       "│ Profile: default                       │\n",
       "│ Pass: actors                           │\n",
       "│ API URL: http://10.10.0.87:8007/api/v1 │\n",
       "╰────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURABLE PROMPT TEMPLATE VARIABLES\n",
    "# =============================================================================\n",
    "\n",
    "# Main configuration for extraction\n",
    "EXTRACTION_MODE = \"ai_enhanced\"  # Options: \"regex\", \"ai_enhanced\", \"hybrid\"\n",
    "EXTRACTION_STRATEGY = \"multipass\"  # Options: \"multipass\", \"ai_enhanced\", \"unified\"\n",
    "EXTRACTION_PROFILE = \"default\"  # Options: \"default\", \"fast\", \"citations\", \"entities\", etc.\n",
    "EXTRACTION_PASS = \"actors\"  # Options: \"actors\", \"citations\", \"concepts\"\n",
    "\n",
    "# Template variables that prompts expect\n",
    "TEMPLATE_VARIABLES = {\n",
    "    \"chunk_content\": \"\",  # The text to process (will be filled dynamically)\n",
    "    \"text\": \"\",  # Alternative name for chunk content\n",
    "    \"text_chunk\": \"\",  # Another alternative name\n",
    "    \"entity_types\": [\"CASE_CITATION\", \"PARTY\", \"ATTORNEY\", \"COURT\", \"JUDGE\"],\n",
    "    \"target_entity_types\": [\"CASE_CITATION\", \"PARTY\", \"ATTORNEY\", \"COURT\", \"JUDGE\"],\n",
    "    \"confidence_threshold\": 0.7,\n",
    "    \"pass_number\": 1,\n",
    "    \"previous_results\": None,\n",
    "    \"max_tokens\": 2000,\n",
    "    \"temperature\": 0.1\n",
    "}\n",
    "\n",
    "# Custom prompt template (optional - set to None to use API templates)\n",
    "CUSTOM_PROMPT_TEMPLATE = None\n",
    "\n",
    "# API endpoint configuration\n",
    "API_BASE_URL = \"http://10.10.0.87:8007/api/v1\"\n",
    "ENTITY_EXTRACTION_PORT = 8007\n",
    "\n",
    "console.print(Panel.fit(\n",
    "    f\"[bold cyan]Extraction Configuration[/bold cyan]\\n\"\n",
    "    f\"Mode: {EXTRACTION_MODE}\\n\"\n",
    "    f\"Strategy: {EXTRACTION_STRATEGY}\\n\"\n",
    "    f\"Profile: {EXTRACTION_PROFILE}\\n\"\n",
    "    f\"Pass: {EXTRACTION_PASS}\\n\"\n",
    "    f\"API URL: {API_BASE_URL}\",\n",
    "    title=\"Configuration\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Loading patterns from API</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mLoading patterns from API\u001b[0m\u001b[1;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">Error loading patterns from API: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">HTTPConnectionPool(</span><span style=\"color: #800000; text-decoration-color: #800000\">host</span><span style=\"color: #800000; text-decoration-color: #800000\">=</span><span style=\"color: #800000; text-decoration-color: #800000\">'10.10.0.87'</span><span style=\"color: #800000; text-decoration-color: #800000\">, </span><span style=\"color: #800000; text-decoration-color: #800000\">port</span><span style=\"color: #800000; text-decoration-color: #800000\">=</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">8007</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span><span style=\"color: #800000; text-decoration-color: #800000\">: Max retries exceeded with url: </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">/api/v1/patterns/detailed </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(</span><span style=\"color: #800000; text-decoration-color: #800000\">Caused by </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">NewConnectionError(</span><span style=\"color: #800000; text-decoration-color: #800000\">'&lt;urllib3.connection.HTTPConnection object at </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">0x7d5e890fbdd0&gt;: Failed to establish a new connection: [Errno 111] Connection refused'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">))</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mError loading patterns from API: \u001b[0m\u001b[1;31mHTTPConnectionPool\u001b[0m\u001b[1;31m(\u001b[0m\u001b[31mhost\u001b[0m\u001b[31m=\u001b[0m\u001b[31m'10.10.0.87'\u001b[0m\u001b[31m, \u001b[0m\u001b[31mport\u001b[0m\u001b[31m=\u001b[0m\u001b[1;31m8007\u001b[0m\u001b[1;31m)\u001b[0m\u001b[31m: Max retries exceeded with url: \u001b[0m\n",
       "\u001b[31m/api/v1/patterns/\u001b[0m\u001b[31mdetailed\u001b[0m\u001b[31m \u001b[0m\u001b[1;31m(\u001b[0m\u001b[31mCaused by \u001b[0m\u001b[1;31mNewConnectionError\u001b[0m\u001b[1;31m(\u001b[0m\u001b[31m'\u001b[0m\u001b[31m<\u001b[0m\u001b[31murllib3.connection.HTTPConnection\u001b[0m\u001b[31m object at \u001b[0m\n",
       "\u001b[31m0x7d5e890fbdd0\u001b[0m\u001b[31m>\u001b[0m\u001b[31m: Failed to establish a new connection: \u001b[0m\u001b[31m[\u001b[0m\u001b[31mErrno 111\u001b[0m\u001b[31m]\u001b[0m\u001b[31m Connection refused'\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Make sure the Entity Extraction Service is running on port </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">8007</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mMake sure the Entity Extraction Service is running on port \u001b[0m\u001b[1;33m8007\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load patterns from Entity Extraction Service API\n",
    "def load_patterns_from_api():\n",
    "    \"\"\"Load patterns from /api/v1/patterns/detailed endpoint\"\"\"\n",
    "    try:\n",
    "        console.print(\"[bold]Loading patterns from API...[/bold]\")\n",
    "        \n",
    "        with console.status(\"[green]Fetching patterns from API...\") as status:\n",
    "            response = requests.get(f\"{API_BASE_URL}/patterns/detailed\")\n",
    "            response.raise_for_status()\n",
    "            patterns_data = response.json()\n",
    "        \n",
    "        console.print(f\"✓ Loaded {patterns_data.get('total_patterns', 0)} patterns\")\n",
    "        \n",
    "        # Display patterns in Rich table\n",
    "        if patterns_data.get('patterns_by_category'):\n",
    "            table = Table(title=\"Available Pattern Categories\", show_lines=True)\n",
    "            table.add_column(\"Category\", style=\"cyan\", width=30)\n",
    "            table.add_column(\"Pattern Count\", style=\"green\", justify=\"right\")\n",
    "            table.add_column(\"Example Pattern\", style=\"yellow\", width=50)\n",
    "            \n",
    "            for category, patterns in patterns_data['patterns_by_category'].items():\n",
    "                if patterns:\n",
    "                    pattern_count = len(patterns)\n",
    "                    # Get first pattern as example\n",
    "                    example = patterns[0]\n",
    "                    pattern_preview = example.get('pattern', '')[:50] + \"...\" if len(example.get('pattern', '')) > 50 else example.get('pattern', '')\n",
    "                    table.add_row(category, str(pattern_count), pattern_preview)\n",
    "            \n",
    "            console.print(table)\n",
    "        \n",
    "        # Show statistics\n",
    "        if patterns_data.get('statistics'):\n",
    "            stats = patterns_data['statistics']\n",
    "            console.print(Panel.fit(\n",
    "                f\"[bold]Pattern Statistics[/bold]\\n\"\n",
    "                f\"Total Patterns: {stats.get('total_patterns', 0)}\\n\"\n",
    "                f\"From YAML Files: {stats.get('patterns_from_yaml', 0)}\\n\"\n",
    "                f\"Entity Types: {len(patterns_data.get('entity_types', []))}\\n\"\n",
    "                f\"Categories: {len(patterns_data.get('patterns_by_category', {}))}\",\n",
    "                title=\"Summary\"\n",
    "            ))\n",
    "        \n",
    "        return patterns_data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        console.print(f\"[red]Error loading patterns from API: {e}[/red]\")\n",
    "        console.print(\"[yellow]Make sure the Entity Extraction Service is running on port 8007[/yellow]\")\n",
    "        return None\n",
    "\n",
    "# Load patterns\n",
    "patterns_data = load_patterns_from_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extraction profiles from API\n",
    "def load_extraction_profiles():\n",
    "    \"\"\"Load extraction profiles from /api/v1/extraction-profiles endpoint\"\"\"\n",
    "    try:\n",
    "        console.print(\"[bold]Loading extraction profiles from API...[/bold]\")\n",
    "        \n",
    "        response = requests.get(f\"{API_BASE_URL}/extraction-profiles\")\n",
    "        response.raise_for_status()\n",
    "        profiles_data = response.json()\n",
    "        \n",
    "        # Display profiles in Rich table\n",
    "        table = Table(title=\"Available Extraction Profiles\", show_lines=True)\n",
    "        table.add_column(\"Profile\", style=\"cyan\", width=20)\n",
    "        table.add_column(\"Description\", style=\"green\", width=40)\n",
    "        table.add_column(\"Enabled Passes\", style=\"yellow\", width=20)\n",
    "        table.add_column(\"Parallel\", style=\"magenta\", justify=\"center\")\n",
    "        \n",
    "        for profile_name, profile_info in profiles_data.get('profiles', {}).items():\n",
    "            passes = str(profile_info.get('enabled_passes', []))\n",
    "            parallel = \"✓\" if profile_info.get('parallel_execution', False) else \"✗\"\n",
    "            table.add_row(\n",
    "                profile_name,\n",
    "                profile_info.get('description', ''),\n",
    "                passes,\n",
    "                parallel\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "        \n",
    "        # Show pass descriptions\n",
    "        if profiles_data.get('pass_descriptions'):\n",
    "            console.print(\"\\n[bold]Pass Descriptions:[/bold]\")\n",
    "            for pass_num, desc in profiles_data['pass_descriptions'].items():\n",
    "                console.print(f\"  Pass {pass_num}: {desc}\")\n",
    "        \n",
    "        return profiles_data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        console.print(f\"[red]Error loading extraction profiles: {e}[/red]\")\n",
    "        return None\n",
    "\n",
    "# Load extraction profiles\n",
    "profiles_data = load_extraction_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template variable discovery and prompt management\n",
    "def discover_template_variables(prompt_template):\n",
    "    \"\"\"Discover what variables a prompt template expects\"\"\"\n",
    "    # Find all {variable_name} and {{variable_name}} patterns\n",
    "    single_brace = re.findall(r'\\{(\\w+)\\}', prompt_template)\n",
    "    double_brace = re.findall(r'\\{\\{(\\w+)\\}\\}', prompt_template)\n",
    "    \n",
    "    all_vars = list(set(single_brace + double_brace))\n",
    "    \n",
    "    console.print(Panel.fit(\n",
    "        f\"[bold]Template expects these variables:[/bold]\\n\" + \n",
    "        \"\\n\".join([f\"  • {var}\" for var in all_vars]),\n",
    "        title=\"Template Variables\"\n",
    "    ))\n",
    "    \n",
    "    return all_vars\n",
    "\n",
    "def get_prompt_template(use_custom=False):\n",
    "    \"\"\"Get the prompt template to use based on configuration\"\"\"\n",
    "    if use_custom and CUSTOM_PROMPT_TEMPLATE:\n",
    "        console.print(\"[yellow]Using custom prompt template[/yellow]\")\n",
    "        return CUSTOM_PROMPT_TEMPLATE\n",
    "    \n",
    "    # Try to get template from loaded data\n",
    "    if template_manager and hasattr(template_manager, 'get_template'):\n",
    "        template = template_manager.get_template(strategy=EXTRACTION_STRATEGY, pass_number=TEMPLATE_VARIABLES.get('pass_number'))\n",
    "        if template:\n",
    "            console.print(f\"[green]Using template from service: {EXTRACTION_STRATEGY}[/green]\")\n",
    "            return template\n",
    "    \n",
    "    # Fallback to minimal template\n",
    "    console.print(\"[yellow]Using fallback minimal template[/yellow]\")\n",
    "    return \"\"\"Extract legal entities from the text.\n",
    "Return as JSON: {{\"entities\": [{{\"type\": \"...\", \"text\": \"...\", \"confidence\": 0.0}}]}}\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "def render_prompt(template: str, text: str, variables: Dict = None) -> str:\n",
    "    \"\"\"Render a prompt template with text and variables\"\"\"\n",
    "    # Merge template variables with provided text\n",
    "    render_vars = TEMPLATE_VARIABLES.copy()\n",
    "    render_vars.update({\n",
    "        'text': text,\n",
    "        'text_chunk': text,\n",
    "        'chunk_content': text\n",
    "    })\n",
    "    \n",
    "    if variables:\n",
    "        render_vars.update(variables)\n",
    "    \n",
    "    # Try Jinja2 rendering first\n",
    "    try:\n",
    "        from jinja2 import Template\n",
    "        jinja_template = Template(template)\n",
    "        return jinja_template.render(**render_vars)\n",
    "    except:\n",
    "        # Fall back to simple format\n",
    "        try:\n",
    "            return template.format(**render_vars)\n",
    "        except KeyError as e:\n",
    "            console.print(f\"[red]Missing template variable: {e}[/red]\")\n",
    "            # Try with just text\n",
    "            return template.format(text=text)\n",
    "\n",
    "# Get and analyze a template\n",
    "selected_template = get_prompt_template()\n",
    "console.print(\"\\n[bold]Selected Template Preview:[/bold]\")\n",
    "console.print(Panel(selected_template[:500] + \"...\" if len(selected_template) > 500 else selected_template))\n",
    "\n",
    "# Discover variables\n",
    "template_vars = discover_template_variables(selected_template)\n",
    "\n",
    "# Test rendering\n",
    "test_text = \"Smith v. Jones, 123 F.3d 456 (9th Cir. 2020)\"\n",
    "test_prompt = render_prompt(selected_template, test_text)\n",
    "console.print(f\"\\n✓ Prompt rendering works\")\n",
    "console.print(f\"✓ Test prompt length: {len(test_prompt)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dynamic Breaking Point Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakingPointDetector:\n",
    "    \"\"\"Detect the character count where extraction fails with detailed progress tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, vllm_controller, template_manager):\n",
    "        self.vllm = vllm_controller\n",
    "        self.templates = template_manager\n",
    "        self.results = []\n",
    "        self.selected_template = get_prompt_template()\n",
    "        \n",
    "    def test_character_count(self, text: str, char_count: int) -> Dict:\n",
    "        \"\"\"Test extraction at specific character count with detailed tracking\"\"\"\n",
    "        \n",
    "        # Show progress for this chunk\n",
    "        console.print(f\"\\n[cyan]Testing {char_count} characters...[/cyan]\")\n",
    "        \n",
    "        # Create chunk of exact size\n",
    "        with console.status(f\"[green]Creating chunk of {char_count} chars...\") as status:\n",
    "            chunk = create_word_boundary_chunk(text, char_count)\n",
    "            actual_size = len(chunk)\n",
    "            console.log(f\"✓ Chunk created: {actual_size} chars\")\n",
    "        \n",
    "        # Render prompt\n",
    "        with console.status(\"[green]Rendering prompt template...\") as status:\n",
    "            prompt = render_prompt(self.selected_template, chunk)\n",
    "            prompt_tokens = len(prompt.split())  # Rough token estimate\n",
    "            console.log(f\"✓ Prompt rendered: ~{prompt_tokens} tokens\")\n",
    "        \n",
    "        # Track performance\n",
    "        start_time = time.time()\n",
    "        success = False\n",
    "        error_msg = None\n",
    "        entities_found = 0\n",
    "        \n",
    "        # Show extraction progress\n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            BarColumn(),\n",
    "            TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
    "            console=console\n",
    "        ) as progress:\n",
    "            \n",
    "            task = progress.add_task(\"[cyan]Extracting entities...\", total=100)\n",
    "            \n",
    "            try:\n",
    "                # Update progress\n",
    "                progress.update(task, advance=20, description=\"[cyan]Calling vLLM...\")\n",
    "                \n",
    "                # Generate with vLLM\n",
    "                response = self.vllm.generate(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=TEMPLATE_VARIABLES['max_tokens'],\n",
    "                    temperature=TEMPLATE_VARIABLES['temperature']\n",
    "                )\n",
    "                \n",
    "                progress.update(task, advance=60, description=\"[cyan]Parsing response...\")\n",
    "                \n",
    "                # Parse response\n",
    "                if response:\n",
    "                    try:\n",
    "                        result = json.loads(response)\n",
    "                        entities_found = len(result.get('entities', []))\n",
    "                        success = entities_found > 0\n",
    "                        progress.update(task, advance=20, description=\"[green]Complete!\")\n",
    "                    except:\n",
    "                        success = False\n",
    "                        error_msg = \"JSON parsing failed\"\n",
    "                        progress.update(task, advance=20, description=\"[red]Parse failed\")\n",
    "            except Exception as e:\n",
    "                success = False\n",
    "                error_msg = str(e)\n",
    "                progress.update(task, advance=80, description=f\"[red]Error: {error_msg[:30]}\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Display result summary\n",
    "        result_panel = Panel.fit(\n",
    "            f\"[bold]Chunk Test Result[/bold]\\n\"\n",
    "            f\"Size: {actual_size} chars\\n\"\n",
    "            f\"Success: {'✓' if success else '✗'}\\n\"\n",
    "            f\"Entities: {entities_found}\\n\"\n",
    "            f\"Time: {elapsed:.2f}s\\n\"\n",
    "            f\"Throughput: {actual_size/elapsed:.0f} chars/s\" if elapsed > 0 else \"\",\n",
    "            title=f\"{'[green]Success' if success else '[red]Failed'}\",\n",
    "            border_style=\"green\" if success else \"red\"\n",
    "        )\n",
    "        console.print(result_panel)\n",
    "        \n",
    "        return {\n",
    "            \"char_count\": char_count,\n",
    "            \"actual_size\": actual_size,\n",
    "            \"success\": success,\n",
    "            \"entities_found\": entities_found,\n",
    "            \"time_seconds\": elapsed,\n",
    "            \"error\": error_msg\n",
    "        }\n",
    "    \n",
    "    def find_breaking_point(self, document: str, start: int = 1000, increment: int = 500, max_size: int = 100000) -> int:\n",
    "        \"\"\"Find the breaking point with detailed progress tracking\"\"\"\n",
    "        \n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold cyan]Finding Breaking Point[/bold cyan]\\n\"\n",
    "            f\"Start: {start:,} chars\\n\"\n",
    "            f\"Increment: {increment} chars\\n\"\n",
    "            f\"Max: {max_size:,} chars\",\n",
    "            title=\"Breaking Point Detection\"\n",
    "        ))\n",
    "        \n",
    "        current_size = start\n",
    "        last_success = 0\n",
    "        first_failure = None\n",
    "        \n",
    "        with Progress(console=console) as progress:\n",
    "            task = progress.add_task(\n",
    "                \"[cyan]Testing chunk sizes...\", \n",
    "                total=min(max_size - start, len(document) - start)\n",
    "            )\n",
    "            \n",
    "            while current_size <= max_size and current_size <= len(document):\n",
    "                result = self.test_character_count(document, current_size)\n",
    "                self.results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    last_success = current_size\n",
    "                    console.print(f\"[green]✓ {current_size:,} chars: Success ({result['entities_found']} entities in {result['time_seconds']:.2f}s)[/green]\")\n",
    "                else:\n",
    "                    if first_failure is None:\n",
    "                        first_failure = current_size\n",
    "                    console.print(f\"[red]✗ {current_size:,} chars: Failed - {result['error']}[/red]\")\n",
    "                    break  # Found breaking point\n",
    "                \n",
    "                progress.update(task, advance=increment)\n",
    "                current_size += increment\n",
    "        \n",
    "        # Binary search for exact breaking point\n",
    "        if first_failure and last_success:\n",
    "            console.print(Panel.fit(\n",
    "                f\"[bold]Narrowing Breaking Point[/bold]\\n\"\n",
    "                f\"Between: {last_success:,} - {first_failure:,} chars\",\n",
    "                title=\"Binary Search\"\n",
    "            ))\n",
    "            breaking_point = self.binary_search(document, last_success, first_failure)\n",
    "            return breaking_point\n",
    "        \n",
    "        return first_failure or last_success\n",
    "    \n",
    "    def binary_search(self, document: str, low: int, high: int) -> int:\n",
    "        \"\"\"Binary search for exact breaking point with progress\"\"\"\n",
    "        iterations = 0\n",
    "        \n",
    "        with Progress(console=console) as progress:\n",
    "            task = progress.add_task(\"[cyan]Binary search...\", total=100)\n",
    "            \n",
    "            while high - low > 10:\n",
    "                mid = (low + high) // 2\n",
    "                iterations += 1\n",
    "                progress.update(task, advance=100/10)  # Estimate 10 iterations max\n",
    "                \n",
    "                result = self.test_character_count(document, mid)\n",
    "                self.results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    low = mid\n",
    "                    console.print(f\"  [green]✓ {mid:,} chars: Success[/green]\")\n",
    "                else:\n",
    "                    high = mid\n",
    "                    console.print(f\"  [red]✗ {mid:,} chars: Failed[/red]\")\n",
    "            \n",
    "            progress.update(task, completed=100)\n",
    "        \n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold green]Breaking Point Found![/bold green]\\n\"\n",
    "            f\"Last successful size: {low:,} chars\\n\"\n",
    "            f\"Binary search iterations: {iterations}\",\n",
    "            title=\"Result\"\n",
    "        ))\n",
    "        \n",
    "        return low  # Last successful size\n",
    "    \n",
    "    def get_results_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Get results as DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# Initialize detector\n",
    "detector = BreakingPointDetector(vllm_controller, template_manager)\n",
    "console.print(\"[green]✓ Breaking point detector initialized with progress tracking[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entity Extraction Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractionStrategy:\n",
    "    \"\"\"Base class for extraction strategies with Rich output\"\"\"\n",
    "\n",
    "    def __init__(self, vllm_controller):\n",
    "        self.vllm = vllm_controller\n",
    "        self.selected_template = get_prompt_template()\n",
    "\n",
    "    def extract(self, text: str) -> Dict:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class UnifiedExtraction(ExtractionStrategy):\n",
    "    \"\"\"Single pass extraction for all entities with detailed tracking\"\"\"\n",
    "\n",
    "    def extract(self, text: str) -> Dict:\n",
    "        console.print(\"\\n[bold cyan]Unified Extraction Strategy[/bold cyan]\")\n",
    "\n",
    "        # Show extraction details\n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold]Extraction Details[/bold]\\n\"\n",
    "            f\"Text size: {len(text):,} chars\\n\"\n",
    "            f\"Strategy: Single unified pass\\n\"\n",
    "            f\"Entity types: All\\n\"\n",
    "            f\"Max tokens: {TEMPLATE_VARIABLES['max_tokens']}\",\n",
    "            title=\"Unified Strategy\"\n",
    "        ))\n",
    "\n",
    "        # Render prompt with template\n",
    "        with console.status(\"[green]Rendering prompt template...\") as status:\n",
    "            prompt = render_prompt(self.selected_template, text)\n",
    "            prompt_size = len(prompt)\n",
    "            console.log(f\"✓ Prompt rendered: {prompt_size:,} chars\")\n",
    "\n",
    "        # Extract with progress tracking\n",
    "        start_time = time.time()\n",
    "\n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            BarColumn(),\n",
    "            TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
    "            console=console\n",
    "        ) as progress:\n",
    "            task = progress.add_task(\"[cyan]Extracting entities...\", total=100)\n",
    "\n",
    "            progress.update(task, advance=30, description=\"[cyan]Calling vLLM...\")\n",
    "            response = self.vllm.generate(\n",
    "                prompt,\n",
    "                max_tokens=TEMPLATE_VARIABLES['max_tokens'],\n",
    "                temperature=TEMPLATE_VARIABLES['temperature']\n",
    "            )\n",
    "\n",
    "            progress.update(task, advance=50, description=\"[cyan]Parsing response...\")\n",
    "            try:\n",
    "                # Try to parse JSON response\n",
    "                if '```json' in response:\n",
    "                    json_str = response.split('```json')[1].split('```')[0]\n",
    "                elif '{' in response:\n",
    "                    start_idx = response.index('{')\n",
    "                    end_idx = response.rindex('}') + 1\n",
    "                    json_str = response[start_idx:end_idx]\n",
    "                else:\n",
    "                    json_str = response\n",
    "\n",
    "                entities = json.loads(json_str)\n",
    "                progress.update(task, advance=20, description=\"[green]Complete!\")\n",
    "            except Exception as e:\n",
    "                entities = {\"entities\": []}\n",
    "                progress.update(task, advance=20, description=\"[yellow]Parse warning\")\n",
    "                console.print(f\"[yellow]Warning: JSON parse issue: {e}[/yellow]\")\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        entity_count = len(entities.get(\"entities\", []))\n",
    "\n",
    "        # Display results summary\n",
    "        result_table = Table(title=\"Unified Extraction Results\")\n",
    "        result_table.add_column(\"Metric\", style=\"cyan\")\n",
    "        result_table.add_column(\"Value\", style=\"green\")\n",
    "\n",
    "        result_table.add_row(\"Entities Found\", str(entity_count))\n",
    "        result_table.add_row(\"Processing Time\", f\"{elapsed:.2f}s\")\n",
    "        result_table.add_row(\"Throughput\", f\"{len(text)/elapsed:.0f} chars/s\")\n",
    "        result_table.add_row(\"Passes\", \"1\")\n",
    "\n",
    "        console.print(result_table)\n",
    "\n",
    "        return {\n",
    "            \"strategy\": \"unified\",\n",
    "            \"entities\": entities.get(\"entities\", []),\n",
    "            \"time\": elapsed,\n",
    "            \"passes\": 1,\n",
    "            \"entity_count\": entity_count\n",
    "        }\n",
    "\n",
    "class MultipassExtraction(ExtractionStrategy):\n",
    "    \"\"\"Multiple specialized passes for different entity groups with Rich tracking\"\"\"\n",
    "\n",
    "    PASSES = [\n",
    "        {\"name\": \"cases_parties\", \"types\": [\"CASE_CITATION\", \"PARTY\", \"ATTORNEY\"]},\n",
    "        {\"name\": \"courts_judges\", \"types\": [\"COURT\", \"JUDGE\"]},\n",
    "        {\"name\": \"citations\", \"types\": [\"USC_CITATION\", \"CFR_CITATION\", \"STATE_STATUTE\"]},\n",
    "        {\"name\": \"dates\", \"types\": [\"DATE\", \"DEADLINE\", \"FILING_DATE\"]},\n",
    "        {\"name\": \"monetary\", \"types\": [\"MONETARY_AMOUNT\", \"DAMAGES\", \"SETTLEMENT\"]},\n",
    "        {\"name\": \"organizations\", \"types\": [\"LAW_FIRM\", \"GOVERNMENT_AGENCY\"]},\n",
    "        {\"name\": \"misc\", \"types\": [\"ADDRESS\", \"PHONE\", \"EMAIL\"]}\n",
    "    ]\n",
    "\n",
    "    def extract(self, text: str) -> Dict:\n",
    "        console.print(\"\\n[bold cyan]Multipass Extraction Strategy[/bold cyan]\")\n",
    "\n",
    "        # Show strategy overview\n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold]Multipass Details[/bold]\\n\"\n",
    "            f\"Text size: {len(text):,} chars\\n\"\n",
    "            f\"Total passes: {len(self.PASSES)}\\n\"\n",
    "            f\"Pass strategy: Specialized entity groups\\n\"\n",
    "            f\"Template: {EXTRACTION_STRATEGY}\",\n",
    "            title=\"Multipass Strategy\"\n",
    "        ))\n",
    "\n",
    "        all_entities = []\n",
    "        total_time = 0\n",
    "        pass_results = []\n",
    "\n",
    "        # Create progress bar for all passes\n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            BarColumn(),\n",
    "            TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
    "            console=console\n",
    "        ) as progress:\n",
    "\n",
    "            main_task = progress.add_task(\n",
    "                \"[cyan]Running multipass extraction...\",\n",
    "                total=len(self.PASSES)\n",
    "            )\n",
    "\n",
    "            for i, pass_config in enumerate(self.PASSES, 1):\n",
    "                progress.update(\n",
    "                    main_task,\n",
    "                    description=f\"[cyan]Pass {i}/{len(self.PASSES)}: {pass_config['name']}...\"\n",
    "                )\n",
    "\n",
    "                # Update template variables for this pass\n",
    "                pass_variables = TEMPLATE_VARIABLES.copy()\n",
    "                pass_variables['pass_number'] = i\n",
    "                pass_variables['entity_types'] = pass_config['types']\n",
    "                pass_variables['target_entity_types'] = pass_config['types']\n",
    "\n",
    "                # Try to get pass-specific template or use configured template\n",
    "                if template_manager and hasattr(template_manager, 'get_template'):\n",
    "                    pass_template = template_manager.get_template(\"multipass\", i)\n",
    "                    if not pass_template:\n",
    "                        pass_template = self.selected_template\n",
    "                else:\n",
    "                    pass_template = self.selected_template\n",
    "\n",
    "                # Render prompt for this pass\n",
    "                prompt = render_prompt(pass_template, text, pass_variables)\n",
    "\n",
    "                # Extract entities for this pass\n",
    "                start_time = time.time()\n",
    "                response = self.vllm.generate(\n",
    "                    prompt,\n",
    "                    max_tokens=1000,\n",
    "                    temperature=TEMPLATE_VARIABLES['temperature']\n",
    "                )\n",
    "                elapsed = time.time() - start_time\n",
    "                total_time += elapsed\n",
    "\n",
    "                # Parse response\n",
    "                pass_entities = []\n",
    "                try:\n",
    "                    if '```json' in response:\n",
    "                        json_str = response.split('```json')[1].split('```')[0]\n",
    "                    elif '{' in response:\n",
    "                        start_idx = response.index('{')\n",
    "                        end_idx = response.rindex('}') + 1\n",
    "                        json_str = response[start_idx:end_idx]\n",
    "                    else:\n",
    "                        json_str = response\n",
    "\n",
    "                    result = json.loads(json_str)\n",
    "                    pass_entities = result.get(\"entities\", [])\n",
    "                    all_entities.extend(pass_entities)\n",
    "                except Exception as e:\n",
    "                    console.print(f\"[yellow]Pass {i} parse warning: {e}[/yellow]\")\n",
    "\n",
    "                # Track pass results\n",
    "                pass_results.append({\n",
    "                    \"pass\": i,\n",
    "                    \"name\": pass_config['name'],\n",
    "                    \"entities_found\": len(pass_entities),\n",
    "                    \"time\": elapsed,\n",
    "                    \"types\": pass_config['types']\n",
    "                })\n",
    "\n",
    "                progress.advance(main_task)\n",
    "\n",
    "        # Display detailed pass results\n",
    "        pass_table = Table(title=\"Multipass Extraction Results by Pass\")\n",
    "        pass_table.add_column(\"Pass\", style=\"cyan\", justify=\"center\")\n",
    "        pass_table.add_column(\"Name\", style=\"green\")\n",
    "        pass_table.add_column(\"Entity Types\", style=\"yellow\")\n",
    "        pass_table.add_column(\"Found\", style=\"magenta\", justify=\"right\")\n",
    "        pass_table.add_column(\"Time (s)\", style=\"blue\", justify=\"right\")\n",
    "\n",
    "        for result in pass_results:\n",
    "            types_str = \", \".join(result['types'][:2]) + (\"...\" if len(result['types']) > 2 else \"\")\n",
    "            pass_table.add_row(\n",
    "                str(result['pass']),\n",
    "                result['name'],\n",
    "                types_str,\n",
    "                str(result['entities_found']),\n",
    "                f\"{result['time']:.2f}\"\n",
    "            )\n",
    "\n",
    "        console.print(pass_table)\n",
    "\n",
    "        # Summary statistics\n",
    "        total_entities = len(all_entities)\n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold green]Multipass Complete![/bold green]\\n\"\n",
    "            f\"Total entities: {total_entities}\\n\"\n",
    "            f\"Total time: {total_time:.2f}s\\n\"\n",
    "            f\"Average per pass: {total_time/len(self.PASSES):.2f}s\\n\"\n",
    "            f\"Throughput: {len(text)/total_time:.0f} chars/s\",\n",
    "            title=\"Summary\"\n",
    "        ))\n",
    "\n",
    "        return {\n",
    "            \"strategy\": \"multipass\",\n",
    "            \"entities\": all_entities,\n",
    "            \"time\": total_time,\n",
    "            \"passes\": len(self.PASSES),\n",
    "            \"entity_count\": total_entities,\n",
    "            \"pass_results\": pass_results\n",
    "        }\n",
    "\n",
    "class IndividualExtraction(ExtractionStrategy):\n",
    "    \"\"\"Separate pass for each individual entity type with detailed tracking\"\"\"\n",
    "\n",
    "    ENTITY_TYPES = [\n",
    "        \"CASE_CITATION\", \"PARTY\", \"ATTORNEY\", \"COURT\", \"JUDGE\",\n",
    "        \"USC_CITATION\", \"CFR_CITATION\", \"STATE_STATUTE\",\n",
    "        \"DATE\", \"DEADLINE\", \"MONETARY_AMOUNT\",\n",
    "        \"LAW_FIRM\", \"GOVERNMENT_AGENCY\"\n",
    "    ]\n",
    "\n",
    "    def extract(self, text: str) -> Dict:\n",
    "        console.print(\"\\n[bold cyan]Individual Entity Extraction Strategy[/bold cyan]\")\n",
    "\n",
    "        # Show strategy overview\n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold]Individual Extraction Details[/bold]\\n\"\n",
    "            f\"Text size: {len(text):,} chars\\n\"\n",
    "            f\"Entity types: {len(self.ENTITY_TYPES)}\\n\"\n",
    "            f\"Strategy: One pass per entity type\\n\"\n",
    "            f\"Max tokens per pass: 500\",\n",
    "            title=\"Individual Strategy\"\n",
    "        ))\n",
    "\n",
    "        all_entities = []\n",
    "        total_time = 0\n",
    "        type_results = []\n",
    "\n",
    "        # Create progress tracking for all entity types\n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            BarColumn(),\n",
    "            TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
    "            console=console\n",
    "        ) as progress:\n",
    "\n",
    "            main_task = progress.add_task(\n",
    "                \"[cyan]Extracting individual entity types...\",\n",
    "                total=len(self.ENTITY_TYPES)\n",
    "            )\n",
    "\n",
    "            for entity_type in self.ENTITY_TYPES:\n",
    "                progress.update(\n",
    "                    main_task,\n",
    "                    description=f\"[cyan]Extracting: {entity_type}...\"\n",
    "                )\n",
    "\n",
    "                # Update template variables for this entity type\n",
    "                type_variables = TEMPLATE_VARIABLES.copy()\n",
    "                type_variables['entity_types'] = [entity_type]\n",
    "                type_variables['target_entity_types'] = [entity_type]\n",
    "\n",
    "                # Create focused prompt for single entity type\n",
    "                if '{{' in self.selected_template or '{%' in self.selected_template:\n",
    "                    # Use template if it has variables\n",
    "                    prompt = render_prompt(self.selected_template, text, type_variables)\n",
    "                else:\n",
    "                    # Use simple focused prompt\n",
    "                    prompt = f\"\"\"Extract only {entity_type} entities.\n",
    "Return as JSON: {{\"entities\": [{{\"type\": \"{entity_type}\", \"text\": \"...\"}}]}}\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "                # Extract entities\n",
    "                start_time = time.time()\n",
    "                response = self.vllm.generate(\n",
    "                    prompt,\n",
    "                    max_tokens=500,\n",
    "                    temperature=TEMPLATE_VARIABLES['temperature']\n",
    "                )\n",
    "                elapsed = time.time() - start_time\n",
    "                total_time += elapsed\n",
    "\n",
    "                # Parse response\n",
    "                type_entities = []\n",
    "                try:\n",
    "                    if '```json' in response:\n",
    "                        json_str = response.split('```json')[1].split('```')[0]\n",
    "                    elif '{' in response:\n",
    "                        start_idx = response.index('{')\n",
    "                        end_idx = response.rindex('}') + 1\n",
    "                        json_str = response[start_idx:end_idx]\n",
    "                    else:\n",
    "                        json_str = response\n",
    "\n",
    "                    result = json.loads(json_str)\n",
    "                    type_entities = result.get(\"entities\", [])\n",
    "                    all_entities.extend(type_entities)\n",
    "                except Exception as e:\n",
    "                    pass  # Silent fail for individual types\n",
    "\n",
    "                # Track results for this type\n",
    "                type_results.append({\n",
    "                    \"type\": entity_type,\n",
    "                    \"found\": len(type_entities),\n",
    "                    \"time\": elapsed\n",
    "                })\n",
    "\n",
    "                progress.advance(main_task)\n",
    "\n",
    "        # Display results by entity type\n",
    "        type_table = Table(title=\"Individual Entity Type Results\")\n",
    "        type_table.add_column(\"Entity Type\", style=\"cyan\", width=20)\n",
    "        type_table.add_column(\"Found\", style=\"green\", justify=\"right\")\n",
    "        type_table.add_column(\"Time (s)\", style=\"yellow\", justify=\"right\")\n",
    "        type_table.add_column(\"Rate\", style=\"magenta\", justify=\"right\")\n",
    "\n",
    "        for result in type_results:\n",
    "            rate = f\"{result['found']/result['time']:.1f}/s\" if result['time'] > 0 else \"N/A\"\n",
    "            type_table.add_row(\n",
    "                result['type'],\n",
    "                str(result['found']),\n",
    "                f\"{result['time']:.2f}\",\n",
    "                rate\n",
    "            )\n",
    "\n",
    "        console.print(type_table)\n",
    "\n",
    "        # Summary\n",
    "        total_entities = len(all_entities)\n",
    "        successful_types = sum(1 for r in type_results if r['found'] > 0)\n",
    "\n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold green]Individual Extraction Complete![/bold green]\\n\"\n",
    "            f\"Total entities: {total_entities}\\n\"\n",
    "            f\"Successful types: {successful_types}/{len(self.ENTITY_TYPES)}\\n\"\n",
    "            f\"Total time: {total_time:.2f}s\\n\"\n",
    "            f\"Average per type: {total_time/len(self.ENTITY_TYPES):.2f}s\",\n",
    "            title=\"Summary\"\n",
    "        ))\n",
    "\n",
    "        return {\n",
    "            \"strategy\": \"individual\",\n",
    "            \"entities\": all_entities,\n",
    "            \"time\": total_time,\n",
    "            \"passes\": len(self.ENTITY_TYPES),\n",
    "            \"entity_count\": total_entities,\n",
    "            \"type_results\": type_results\n",
    "        }\n",
    "\n",
    "console.print(\"[green]✓ Extraction strategies defined with Rich console output[/green]\")\n",
    "console.print(\"  - [cyan]Unified[/cyan]: Single pass for all entities with progress tracking\")\n",
    "console.print(\"  - [cyan]Multipass[/cyan]: 7 specialized passes with detailed per-pass results\")\n",
    "console.print(\"  - [cyan]Individual[/cyan]: Separate pass per entity type with type-by-type tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Visualization functions ready\n"
     ]
    }
   ],
   "source": [
    "def plot_breaking_point_analysis(results_df: pd.DataFrame):\n",
    "    \"\"\"Plot breaking point analysis\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Success rate by character count\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(results_df['char_count'], results_df['success'].astype(int), 'o-')\n",
    "    ax.set_xlabel('Character Count')\n",
    "    ax.set_ylabel('Success (1) / Failure (0)')\n",
    "    ax.set_title('Extraction Success by Character Count')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Processing time by character count\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(results_df['char_count'], results_df['time_seconds'], 'o-', color='orange')\n",
    "    ax.set_xlabel('Character Count')\n",
    "    ax.set_ylabel('Processing Time (seconds)')\n",
    "    ax.set_title('Processing Time vs Character Count')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Entities found by character count\n",
    "    ax = axes[1, 0]\n",
    "    successful = results_df[results_df['success'] == True]\n",
    "    if not successful.empty:\n",
    "        ax.plot(successful['char_count'], successful['entities_found'], 'o-', color='green')\n",
    "    ax.set_xlabel('Character Count')\n",
    "    ax.set_ylabel('Number of Entities')\n",
    "    ax.set_title('Entities Found vs Character Count')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Characters per second (throughput)\n",
    "    ax = axes[1, 1]\n",
    "    if not successful.empty:\n",
    "        throughput = successful['char_count'] / successful['time_seconds']\n",
    "        ax.plot(successful['char_count'], throughput, 'o-', color='purple')\n",
    "    ax.set_xlabel('Character Count')\n",
    "    ax.set_ylabel('Characters per Second')\n",
    "    ax.set_title('Processing Throughput')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_strategy_comparison(strategy_results: List[Dict]):\n",
    "    \"\"\"Compare different extraction strategies\"\"\"\n",
    "    df = pd.DataFrame(strategy_results)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Entities extracted\n",
    "    ax = axes[0]\n",
    "    strategies = df['strategy'].unique()\n",
    "    entity_counts = [df[df['strategy'] == s]['entity_count'].mean() for s in strategies]\n",
    "    ax.bar(strategies, entity_counts)\n",
    "    ax.set_ylabel('Average Entities Extracted')\n",
    "    ax.set_title('Extraction Effectiveness')\n",
    "    \n",
    "    # Processing time\n",
    "    ax = axes[1]\n",
    "    times = [df[df['strategy'] == s]['time'].mean() for s in strategies]\n",
    "    ax.bar(strategies, times, color='orange')\n",
    "    ax.set_ylabel('Average Time (seconds)')\n",
    "    ax.set_title('Processing Time')\n",
    "    \n",
    "    # Efficiency (entities per second)\n",
    "    ax = axes[2]\n",
    "    efficiency = [e/t if t > 0 else 0 for e, t in zip(entity_counts, times)]\n",
    "    ax.bar(strategies, efficiency, color='green')\n",
    "    ax.set_ylabel('Entities per Second')\n",
    "    ax.set_title('Extraction Efficiency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"✓ Visualization functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Automated Testing Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatedTester:\n",
    "    \"\"\"Automated testing for entity extraction with detailed processing inspection\"\"\"\n",
    "    \n",
    "    def __init__(self, vllm_controller):\n",
    "        self.vllm = vllm_controller\n",
    "        self.detector = BreakingPointDetector(vllm_controller, template_manager)\n",
    "        self.results = []\n",
    "        \n",
    "    def load_test_document(self, path: str = \"/srv/luris/be/tests/docs/Rahimi.pdf\") -> str:\n",
    "        \"\"\"Load test document with progress tracking\"\"\"\n",
    "        console.print(f\"[bold]Loading document: {path}[/bold]\")\n",
    "        \n",
    "        # For now, use sample text\n",
    "        # In production, would load actual document\n",
    "        sample = \"\"\"\n",
    "        In the case of Smith v. Jones, 123 F.3d 456 (9th Cir. 2020), the United States Court of Appeals \n",
    "        for the Ninth Circuit held that the defendant, ABC Corporation, represented by attorney John Doe \n",
    "        of the law firm Doe & Associates, violated 42 U.S.C. § 1983. The case was presided over by \n",
    "        Judge Jane Smith. The plaintiff, Robert Smith, sought damages of $1,000,000 for violations that \n",
    "        occurred on January 15, 2019. The court's decision was issued on March 20, 2020, following oral \n",
    "        arguments held on February 10, 2020. The defendant filed a motion to dismiss on November 1, 2019, \n",
    "        which was denied by the district court. This decision affirmed the lower court's ruling in \n",
    "        Smith v. ABC Corp., 456 F. Supp. 3d 789 (N.D. Cal. 2019).\n",
    "        \"\"\" * 50  # Repeat to create longer document\n",
    "        \n",
    "        # Display document info\n",
    "        doc_table = Table(title=\"Document Information\")\n",
    "        doc_table.add_column(\"Property\", style=\"cyan\")\n",
    "        doc_table.add_column(\"Value\", style=\"green\")\n",
    "        \n",
    "        doc_table.add_row(\"Path\", path)\n",
    "        doc_table.add_row(\"Length\", f\"{len(sample):,} characters\")\n",
    "        doc_table.add_row(\"Estimated Tokens\", f\"~{len(sample)//4:,}\")\n",
    "        doc_table.add_row(\"Lines\", str(sample.count('\\n') + 1))\n",
    "        doc_table.add_row(\"Sample Entities\", \"CASE_CITATION, PARTY, COURT, etc.\")\n",
    "        \n",
    "        console.print(doc_table)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def inspect_extraction_process(self, text: str, chunk_size: int = 4000):\n",
    "        \"\"\"Detailed inspection of the extraction process\"\"\"\n",
    "        console.print(Panel.fit(\n",
    "            \"[bold cyan]Extraction Process Inspector[/bold cyan]\\n\"\n",
    "            \"This will show detailed step-by-step processing\",\n",
    "            title=\"Inspector\"\n",
    "        ))\n",
    "        \n",
    "        # Step 1: Chunking\n",
    "        console.print(\"\\n[bold]Step 1: Document Chunking[/bold]\")\n",
    "        chunk = create_word_boundary_chunk(text, chunk_size)\n",
    "        \n",
    "        chunk_info = Table(title=\"Chunk Information\")\n",
    "        chunk_info.add_column(\"Metric\", style=\"cyan\")\n",
    "        chunk_info.add_column(\"Value\", style=\"green\")\n",
    "        \n",
    "        chunk_info.add_row(\"Requested Size\", f\"{chunk_size:,} chars\")\n",
    "        chunk_info.add_row(\"Actual Size\", f\"{len(chunk):,} chars\")\n",
    "        chunk_info.add_row(\"Word Count\", f\"{len(chunk.split()):,}\")\n",
    "        chunk_info.add_row(\"Sentence Count\", f\"{chunk.count('.'):,}\")\n",
    "        \n",
    "        console.print(chunk_info)\n",
    "        \n",
    "        # Step 2: Template Selection\n",
    "        console.print(\"\\n[bold]Step 2: Template Selection & Configuration[/bold]\")\n",
    "        template = get_prompt_template()\n",
    "        template_vars = discover_template_variables(template)\n",
    "        \n",
    "        template_table = Table(title=\"Template Configuration\")\n",
    "        template_table.add_column(\"Setting\", style=\"cyan\")\n",
    "        template_table.add_column(\"Value\", style=\"green\")\n",
    "        \n",
    "        template_table.add_row(\"Strategy\", EXTRACTION_STRATEGY)\n",
    "        template_table.add_row(\"Profile\", EXTRACTION_PROFILE)\n",
    "        template_table.add_row(\"Template Variables\", str(len(template_vars)))\n",
    "        template_table.add_row(\"Max Tokens\", str(TEMPLATE_VARIABLES['max_tokens']))\n",
    "        template_table.add_row(\"Temperature\", str(TEMPLATE_VARIABLES['temperature']))\n",
    "        \n",
    "        console.print(template_table)\n",
    "        \n",
    "        # Step 3: Prompt Rendering\n",
    "        console.print(\"\\n[bold]Step 3: Prompt Rendering[/bold]\")\n",
    "        with console.status(\"[green]Rendering prompt...\") as status:\n",
    "            prompt = render_prompt(template, chunk)\n",
    "            prompt_lines = prompt.split('\\n')\n",
    "            \n",
    "        prompt_info = Table(title=\"Rendered Prompt Analysis\")\n",
    "        prompt_info.add_column(\"Metric\", style=\"cyan\")\n",
    "        prompt_info.add_column(\"Value\", style=\"green\")\n",
    "        \n",
    "        prompt_info.add_row(\"Total Length\", f\"{len(prompt):,} chars\")\n",
    "        prompt_info.add_row(\"Line Count\", str(len(prompt_lines)))\n",
    "        prompt_info.add_row(\"Estimated Tokens\", f\"~{len(prompt)//4:,}\")\n",
    "        prompt_info.add_row(\"Contains Examples\", \"Yes\" if \"example\" in prompt.lower() else \"No\")\n",
    "        prompt_info.add_row(\"JSON Format\", \"Yes\" if \"json\" in prompt.lower() else \"No\")\n",
    "        \n",
    "        console.print(prompt_info)\n",
    "        \n",
    "        # Show prompt preview\n",
    "        console.print(\"\\n[bold]Prompt Preview (first 500 chars):[/bold]\")\n",
    "        console.print(Panel(\n",
    "            prompt[:500] + \"...\" if len(prompt) > 500 else prompt,\n",
    "            title=\"Prompt\",\n",
    "            border_style=\"dim\"\n",
    "        ))\n",
    "        \n",
    "        # Step 4: Model Inference\n",
    "        console.print(\"\\n[bold]Step 4: Model Inference[/bold]\")\n",
    "        \n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            BarColumn(),\n",
    "            TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
    "            console=console\n",
    "        ) as progress:\n",
    "            \n",
    "            task = progress.add_task(\"[cyan]Running inference...\", total=100)\n",
    "            \n",
    "            progress.update(task, advance=20, description=\"[cyan]Sending to vLLM...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Simulate extraction (in production, would call actual vLLM)\n",
    "            progress.update(task, advance=40, description=\"[cyan]Processing tokens...\")\n",
    "            time.sleep(0.5)  # Simulate processing\n",
    "            \n",
    "            progress.update(task, advance=30, description=\"[cyan]Generating response...\")\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            progress.update(task, advance=10, description=\"[green]Complete!\")\n",
    "        \n",
    "        # Step 5: Response Analysis\n",
    "        console.print(\"\\n[bold]Step 5: Response Analysis[/bold]\")\n",
    "        \n",
    "        # Simulate response\n",
    "        mock_response = {\n",
    "            \"entities\": [\n",
    "                {\"type\": \"CASE_CITATION\", \"text\": \"Smith v. Jones, 123 F.3d 456 (9th Cir. 2020)\"},\n",
    "                {\"type\": \"PARTY\", \"text\": \"ABC Corporation\"},\n",
    "                {\"type\": \"ATTORNEY\", \"text\": \"John Doe\"},\n",
    "                {\"type\": \"COURT\", \"text\": \"Ninth Circuit\"}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response_table = Table(title=\"Response Metrics\")\n",
    "        response_table.add_column(\"Metric\", style=\"cyan\")\n",
    "        response_table.add_column(\"Value\", style=\"green\")\n",
    "        \n",
    "        response_table.add_row(\"Processing Time\", f\"{elapsed:.2f}s\")\n",
    "        response_table.add_row(\"Entities Found\", str(len(mock_response['entities'])))\n",
    "        response_table.add_row(\"Unique Types\", str(len(set(e['type'] for e in mock_response['entities']))))\n",
    "        response_table.add_row(\"Throughput\", f\"{chunk_size/elapsed:.0f} chars/s\")\n",
    "        \n",
    "        console.print(response_table)\n",
    "        \n",
    "        # Entity breakdown\n",
    "        entity_types = {}\n",
    "        for entity in mock_response['entities']:\n",
    "            entity_type = entity['type']\n",
    "            entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "        \n",
    "        entity_table = Table(title=\"Entities by Type\")\n",
    "        entity_table.add_column(\"Type\", style=\"cyan\")\n",
    "        entity_table.add_column(\"Count\", style=\"green\", justify=\"right\")\n",
    "        entity_table.add_column(\"Example\", style=\"yellow\")\n",
    "        \n",
    "        for entity_type, count in entity_types.items():\n",
    "            example = next((e['text'] for e in mock_response['entities'] if e['type'] == entity_type), \"\")\n",
    "            if len(example) > 40:\n",
    "                example = example[:37] + \"...\"\n",
    "            entity_table.add_row(entity_type, str(count), example)\n",
    "        \n",
    "        console.print(entity_table)\n",
    "        \n",
    "        return {\n",
    "            \"chunk_size\": len(chunk),\n",
    "            \"prompt_size\": len(prompt),\n",
    "            \"processing_time\": elapsed,\n",
    "            \"entities_found\": len(mock_response['entities']),\n",
    "            \"entity_types\": entity_types\n",
    "        }\n",
    "    \n",
    "    def run_comprehensive_test(self, document: str, model_config: VLLMConfig):\n",
    "        \"\"\"Run comprehensive test suite with detailed inspection\"\"\"\n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold cyan]Comprehensive Test Suite[/bold cyan]\\n\"\n",
    "            f\"Model: {model_config.model_name}\\n\"\n",
    "            f\"Document: {len(document):,} characters\",\n",
    "            title=\"Test Suite\"\n",
    "        ))\n",
    "        \n",
    "        # Test 1: Process Inspection\n",
    "        console.print(\"\\n[bold yellow]═══ Test 1: Process Inspection ═══[/bold yellow]\")\n",
    "        inspection_result = self.inspect_extraction_process(document, 4000)\n",
    "        \n",
    "        # Test 2: Find breaking point\n",
    "        console.print(\"\\n[bold yellow]═══ Test 2: Breaking Point Detection ═══[/bold yellow]\")\n",
    "        breaking_point = self.detector.find_breaking_point(\n",
    "            document, \n",
    "            start=1000,\n",
    "            increment=500,\n",
    "            max_size=50000\n",
    "        )\n",
    "        \n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold green]Breaking Point Found![/bold green]\\n\"\n",
    "            f\"Maximum successful size: {breaking_point:,} characters\",\n",
    "            title=\"Result\"\n",
    "        ))\n",
    "        \n",
    "        # Test 3: Compare strategies at safe size\n",
    "        safe_size = min(breaking_point - 100, 4000)\n",
    "        console.print(f\"\\n[bold yellow]═══ Test 3: Strategy Comparison ({safe_size:,} chars) ═══[/bold yellow]\")\n",
    "        \n",
    "        strategies = [\n",
    "            UnifiedExtraction(self.vllm),\n",
    "            MultipassExtraction(self.vllm),\n",
    "            IndividualExtraction(self.vllm)\n",
    "        ]\n",
    "        \n",
    "        strategy_results = []\n",
    "        for strategy in strategies:\n",
    "            chunk = create_word_boundary_chunk(document, safe_size)\n",
    "            console.print(f\"\\n[bold]Testing {strategy.__class__.__name__}...[/bold]\")\n",
    "            result = strategy.extract(chunk)\n",
    "            result['char_count'] = len(chunk)\n",
    "            strategy_results.append(result)\n",
    "        \n",
    "        # Compare strategies\n",
    "        comparison_table = Table(title=\"Strategy Comparison\")\n",
    "        comparison_table.add_column(\"Strategy\", style=\"cyan\")\n",
    "        comparison_table.add_column(\"Entities\", style=\"green\", justify=\"right\")\n",
    "        comparison_table.add_column(\"Time (s)\", style=\"yellow\", justify=\"right\")\n",
    "        comparison_table.add_column(\"Passes\", style=\"magenta\", justify=\"right\")\n",
    "        comparison_table.add_column(\"Efficiency\", style=\"blue\", justify=\"right\")\n",
    "        \n",
    "        for result in strategy_results:\n",
    "            efficiency = f\"{result['entity_count']/result['time']:.1f} ent/s\" if result['time'] > 0 else \"N/A\"\n",
    "            comparison_table.add_row(\n",
    "                result['strategy'].title(),\n",
    "                str(result['entity_count']),\n",
    "                f\"{result['time']:.2f}\",\n",
    "                str(result['passes']),\n",
    "                efficiency\n",
    "            )\n",
    "        \n",
    "        console.print(comparison_table)\n",
    "        \n",
    "        # Test 4: Performance at different sizes\n",
    "        console.print(f\"\\n[bold yellow]═══ Test 4: Performance Scaling ═══[/bold yellow]\")\n",
    "        test_sizes = [1000, 2000, 3000, 4000, safe_size]\n",
    "        perf_results = []\n",
    "        \n",
    "        perf_table = Table(title=\"Performance at Different Sizes\")\n",
    "        perf_table.add_column(\"Size\", style=\"cyan\", justify=\"right\")\n",
    "        perf_table.add_column(\"Status\", style=\"green\", justify=\"center\")\n",
    "        perf_table.add_column(\"Time (s)\", style=\"yellow\", justify=\"right\")\n",
    "        perf_table.add_column(\"Entities\", style=\"magenta\", justify=\"right\")\n",
    "        perf_table.add_column(\"Rate\", style=\"blue\", justify=\"right\")\n",
    "        \n",
    "        for size in test_sizes:\n",
    "            if size <= breaking_point:\n",
    "                result = self.detector.test_character_count(document, size)\n",
    "                perf_results.append(result)\n",
    "                \n",
    "                status = \"✓\" if result['success'] else \"✗\"\n",
    "                rate = f\"{size/result['time_seconds']:.0f} c/s\" if result['time_seconds'] > 0 else \"N/A\"\n",
    "                \n",
    "                perf_table.add_row(\n",
    "                    f\"{size:,}\",\n",
    "                    status,\n",
    "                    f\"{result['time_seconds']:.2f}\",\n",
    "                    str(result['entities_found']),\n",
    "                    rate\n",
    "                )\n",
    "        \n",
    "        console.print(perf_table)\n",
    "        \n",
    "        return {\n",
    "            \"breaking_point\": breaking_point,\n",
    "            \"strategy_results\": strategy_results,\n",
    "            \"performance_results\": perf_results,\n",
    "            \"model\": model_config.model_name,\n",
    "            \"inspection_result\": inspection_result\n",
    "        }\n",
    "    \n",
    "    def generate_report(self, test_results: Dict):\n",
    "        \"\"\"Generate comprehensive test report with Rich formatting\"\"\"\n",
    "        \n",
    "        console.print(\"\\n[bold cyan]═══ Final Test Report ═══[/bold cyan]\")\n",
    "        \n",
    "        # Model Configuration\n",
    "        console.print(Panel.fit(\n",
    "            f\"[bold]Model Configuration[/bold]\\n\"\n",
    "            f\"Model: {test_results['model']}\\n\"\n",
    "            f\"Breaking Point: {test_results['breaking_point']:,} characters\",\n",
    "            title=\"Configuration\"\n",
    "        ))\n",
    "        \n",
    "        # Strategy Comparison\n",
    "        console.print(\"\\n[bold]Strategy Performance Summary:[/bold]\")\n",
    "        for result in test_results['strategy_results']:\n",
    "            console.print(f\"  • {result['strategy'].title()}: {result['entity_count']} entities in {result['time']:.2f}s ({result['passes']} passes)\")\n",
    "        \n",
    "        # Performance Analysis\n",
    "        if test_results['performance_results']:\n",
    "            successful = [r for r in test_results['performance_results'] if r['success']]\n",
    "            if successful:\n",
    "                avg_time = sum(r['time_seconds'] for r in successful) / len(successful)\n",
    "                avg_entities = sum(r['entities_found'] for r in successful) / len(successful)\n",
    "                \n",
    "                console.print(Panel.fit(\n",
    "                    f\"[bold]Performance Statistics[/bold]\\n\"\n",
    "                    f\"Tests Run: {len(test_results['performance_results'])}\\n\"\n",
    "                    f\"Successful: {len(successful)}\\n\"\n",
    "                    f\"Average Time: {avg_time:.2f}s\\n\"\n",
    "                    f\"Average Entities: {avg_entities:.1f}\",\n",
    "                    title=\"Statistics\"\n",
    "                ))\n",
    "        \n",
    "        # Recommendations\n",
    "        console.print(\"\\n[bold]Recommendations:[/bold]\")\n",
    "        console.print(f\"  • Optimal chunk size: {test_results['breaking_point'] - 500:,} characters\")\n",
    "        \n",
    "        # Find best strategy\n",
    "        best_strategy = min(test_results['strategy_results'], key=lambda x: x['time']/max(x['entity_count'], 1))\n",
    "        console.print(f\"  • Best strategy: {best_strategy['strategy'].title()}\")\n",
    "        console.print(f\"  • Use chunking for documents > {test_results['breaking_point']:,} characters\")\n",
    "        \n",
    "        return \"Report generated successfully\"\n",
    "\n",
    "# Initialize tester\n",
    "tester = AutomatedTester(vllm_controller)\n",
    "console.print(\"[green]✓ Automated tester initialized with detailed inspection[/green]\")\n",
    "console.print(\"[green]✓ Ready to run comprehensive tests with full process visibility[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Testing Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\"> Current Test Configuration </span>\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Parameter     </span>┃<span style=\"font-weight: bold\"> Value    </span>┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> start_size    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 1000     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> increment     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 500      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> max_size      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 50000    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> model         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> granite  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> config_preset </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> balanced </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_document </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> None     </span>│\n",
       "└───────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m Current Test Configuration \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mParameter    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mstart_size   \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1000    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mincrement    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m500     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mmax_size     \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m50000   \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mmodel        \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mgranite \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mconfig_preset\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mbalanced\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_document\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mNone    \u001b[0m\u001b[32m \u001b[0m│\n",
       "└───────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test parameters configured\n",
      "✓ Ready to start testing\n"
     ]
    }
   ],
   "source": [
    "# Main testing parameters\n",
    "TEST_PARAMS = {\n",
    "    \"start_size\": 1000,\n",
    "    \"increment\": 500,\n",
    "    \"max_size\": 50000,\n",
    "    \"model\": \"granite\",  # or \"qwen\"\n",
    "    \"config_preset\": \"balanced\",\n",
    "    \"test_document\": None,  # Will load default\n",
    "}\n",
    "\n",
    "# Display current configuration\n",
    "table = Table(title=\"Current Test Configuration\")\n",
    "table.add_column(\"Parameter\", style=\"cyan\")\n",
    "table.add_column(\"Value\", style=\"green\")\n",
    "\n",
    "for key, value in TEST_PARAMS.items():\n",
    "    table.add_row(key, str(value))\n",
    "\n",
    "console.print(table)\n",
    "print(\"✓ Test parameters configured\")\n",
    "print(\"✓ Ready to start testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run complete test with detailed inspection\n",
    "def run_breaking_point_test(\n",
    "    start_size: int = 1000,\n",
    "    increment: int = 500,\n",
    "    max_size: int = 50000,\n",
    "    model: str = \"granite\"\n",
    "):\n",
    "    \"\"\"Run complete breaking point test with detailed inspection\"\"\"\n",
    "    \n",
    "    # Load test document\n",
    "    console.print(\"[bold cyan]═══ Loading Test Document ═══[/bold cyan]\")\n",
    "    document = tester.load_test_document()\n",
    "    \n",
    "    # Select model configuration\n",
    "    if model == \"granite\":\n",
    "        model_config = GRANITE_CONFIG\n",
    "    else:\n",
    "        model_config = QWEN_CONFIG\n",
    "    \n",
    "    # Display test configuration\n",
    "    config_table = Table(title=\"Test Configuration\")\n",
    "    config_table.add_column(\"Parameter\", style=\"cyan\")\n",
    "    config_table.add_column(\"Value\", style=\"green\")\n",
    "    \n",
    "    config_table.add_row(\"Model\", model_config.model_name)\n",
    "    config_table.add_row(\"Start Size\", f\"{start_size:,} chars\")\n",
    "    config_table.add_row(\"Increment\", f\"{increment:,} chars\")\n",
    "    config_table.add_row(\"Max Size\", f\"{max_size:,} chars\")\n",
    "    config_table.add_row(\"Extraction Strategy\", EXTRACTION_STRATEGY)\n",
    "    config_table.add_row(\"Extraction Profile\", EXTRACTION_PROFILE)\n",
    "    \n",
    "    console.print(config_table)\n",
    "    \n",
    "    # Load model (in production)\n",
    "    console.print(f\"\\n[bold]Loading {model} model...[/bold]\")\n",
    "    console.print(\"[yellow]Note: In production, this would load the actual vLLM model[/yellow]\")\n",
    "    console.print(\"[yellow]For testing, using mock responses[/yellow]\")\n",
    "    \n",
    "    # Run comprehensive test with inspection\n",
    "    test_results = tester.run_comprehensive_test(document, model_config)\n",
    "    \n",
    "    # Generate and display report\n",
    "    report = tester.generate_report(test_results)\n",
    "    \n",
    "    # Plot results if available\n",
    "    if detector.results:\n",
    "        results_df = detector.get_results_df()\n",
    "        console.print(\"\\n[bold]Generating Performance Plots...[/bold]\")\n",
    "        fig = plot_breaking_point_analysis(results_df)\n",
    "        plt.show()\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Function to run just the inspection\n",
    "def run_process_inspection(text_size: int = 4000):\n",
    "    \"\"\"Run detailed process inspection for a specific text size\"\"\"\n",
    "    \n",
    "    console.print(Panel.fit(\n",
    "        \"[bold cyan]Process Inspection Demo[/bold cyan]\\n\"\n",
    "        f\"This will show detailed extraction process for {text_size:,} chars\",\n",
    "        title=\"Demo\"\n",
    "    ))\n",
    "    \n",
    "    # Load document\n",
    "    document = tester.load_test_document()\n",
    "    \n",
    "    # Run inspection\n",
    "    inspection_result = tester.inspect_extraction_process(document, text_size)\n",
    "    \n",
    "    console.print(Panel.fit(\n",
    "        \"[bold green]Inspection Complete![/bold green]\\n\"\n",
    "        f\"Chunk size: {inspection_result['chunk_size']:,} chars\\n\"\n",
    "        f\"Prompt size: {inspection_result['prompt_size']:,} chars\\n\"\n",
    "        f\"Processing time: {inspection_result['processing_time']:.2f}s\\n\"\n",
    "        f\"Entities found: {inspection_result['entities_found']}\",\n",
    "        title=\"Results\"\n",
    "    ))\n",
    "    \n",
    "    return inspection_result\n",
    "\n",
    "console.print(\"[green]✓ Test runner functions ready[/green]\")\n",
    "console.print(\"\\n[bold]Available test functions:[/bold]\")\n",
    "console.print(\"  • [cyan]run_breaking_point_test()[/cyan] - Run complete test suite with breaking point detection\")\n",
    "console.print(\"  • [cyan]run_process_inspection(4000)[/cyan] - Inspect extraction process for specific size\")\n",
    "console.print(\"  • [cyan]quick_test()[/cyan] - Run quick test with small document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Running Quick Test</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mRunning Quick Test\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Document size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2180</span> characters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Document size: \u001b[1;36m2180\u001b[0m characters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> chars: chunk has <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99</span> chars\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing \u001b[1;36m100\u001b[0m chars: chunk has \u001b[1;36m99\u001b[0m chars\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span> chars: chunk has <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">490</span> chars\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing \u001b[1;36m500\u001b[0m chars: chunk has \u001b[1;36m490\u001b[0m chars\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span> chars: chunk has <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">996</span> chars\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing \u001b[1;36m1000\u001b[0m chars: chunk has \u001b[1;36m996\u001b[0m chars\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1500</span> chars: chunk has <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1497</span> chars\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing \u001b[1;36m1500\u001b[0m chars: chunk has \u001b[1;36m1497\u001b[0m chars\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✓ Quick test complete\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✓ Quick test complete\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick test with small document\n",
    "def quick_test():\n",
    "    \"\"\"Run a quick test with small document\"\"\"\n",
    "    test_doc = \"\"\"Smith v. Jones, 123 F.3d 456 (9th Cir. 2020). \n",
    "    The defendant ABC Corporation was represented by John Doe.\"\"\" * 20\n",
    "    \n",
    "    console.print(\"[bold cyan]Running Quick Test[/bold cyan]\")\n",
    "    console.print(f\"Document size: {len(test_doc)} characters\")\n",
    "    \n",
    "    # Test extraction\n",
    "    chunk_sizes = [100, 500, 1000, 1500]\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        if size <= len(test_doc):\n",
    "            chunk = create_word_boundary_chunk(test_doc, size)\n",
    "            console.print(f\"Testing {size} chars: chunk has {len(chunk)} chars\")\n",
    "            # In production, would actually extract entities here\n",
    "    \n",
    "    console.print(\"✓ Quick test complete\")\n",
    "\n",
    "# Run quick test\n",
    "quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run Process Inspection Demo\n",
    "console.print(\"[bold magenta]═══ Running Process Inspection Demo ═══[/bold magenta]\")\n",
    "\n",
    "# This demonstrates the detailed process inspection\n",
    "# It shows every step of the extraction process with Rich console output\n",
    "inspection_result = run_process_inspection(2000)\n",
    "\n",
    "console.print(\"\\n[bold green]Demo Complete![/bold green]\")\n",
    "console.print(\"[yellow]To run full breaking point test, execute: run_breaking_point_test()[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook provides:\n",
    "1. **Complete vLLM control** using Python API (not HTTP service)\n",
    "2. **Dynamic breaking point detection** starting from 1000 characters\n",
    "3. **Chunking before extraction** for proper document processing\n",
    "4. **Multiple extraction strategies** for comparison\n",
    "5. **Comprehensive testing and analysis** tools\n",
    "\n",
    "### To use this notebook:\n",
    "1. Ensure vLLM is installed and models are available\n",
    "2. Load your test document\n",
    "3. Run `run_breaking_point_test()` to find the breaking point\n",
    "4. Analyze results and optimize parameters\n",
    "5. Test different strategies and configurations\n",
    "\n",
    "### Key findings will include:\n",
    "- Exact character count where extraction fails\n",
    "- Optimal extraction strategy for different document sizes\n",
    "- Performance metrics and bottlenecks\n",
    "- Best vLLM configuration for your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
